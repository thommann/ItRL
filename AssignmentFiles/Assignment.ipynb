{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02944396",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from Chess_env import *\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "BOARD_SIZE = 4\n",
    "NUMBER_OF_EPISODES = 10_000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "0bceca7c",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "\n",
    "You can find the environment in the file Chess_env, which contains the class Chess_env. To define an object, you need to provide the board size considered as input. In our example, size_board=4. \n",
    "Chess_env is composed by the following methods:\n",
    "\n",
    "1. Initialise_game. The method initialises an episode by placing the three pieces considered (Agent's king and queen, enemy's king) in the chess board. The outputs of the method are described below in order.\n",
    "\n",
    "     S $\\;$ A matrix representing the board locations filled with 4 numbers: 0, no piece in that position; 1, location of the \n",
    "     agent's king; 2 location of the queen; 3 location of the enemy king.\n",
    "     \n",
    "     X $\\;$ The features, that is the input to the neural network. See the assignment for more information regarding the            definition of the features adopted. To personalise this, go into the Features method of the class Chess_env() and change        accordingly.\n",
    "     \n",
    "     allowed_a $\\;$ The allowed actions that the agent can make. The agent is moving a king, with a total number of 8                possible actions, and a queen, with a total number of $(board_{size}-1)\\times 8$ actions. The total number of possible actions correspond      to the sum of the two, but not all actions are allowed in a given position (movements to locations outside the borders or      against chess rules). Thus, the variable allowed_a is a vector that is one (zero) for an action that the agent can (can't)      make. Be careful, apply the policy considered on the actions that are allowed only.\n",
    "     \n",
    "\n",
    "2. OneStep. The method performs a one step update of the system. Given as input the action selected by the agent, it updates the chess board by performing that action and the response of the enemy king (which is a random allowed action in the settings considered). The first three outputs are the same as for the Initialise_game method, but the variables are computed for the position reached after the update of the system. The fourth and fifth outputs are:\n",
    "\n",
    "     R $\\;$ The reward. To change this, look at the OneStep method of the class where the rewards are set.\n",
    "     \n",
    "     Done $\\;$ A variable that is 1 if the episode has ended (checkmate or draw).\n",
    "     \n",
    "     \n",
    "3. Features. Given the chessboard position, the method computes the features.\n",
    "\n",
    "This information and a quick analysis of the class should be all you need to get going. The other functions that the class exploits are uncommented and constitute an example on how not to write a python code. You can take a look at them if you want, but it is not necessary.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9593a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITIALISE THE ENVIRONMENT\n",
    "regular_chess_environment = Chess_Env(BOARD_SIZE)\n",
    "regular_board, regular_X, regular_allowed_a = regular_chess_environment.Initialise_game()\n",
    "regular_n_possible_actions = np.shape(regular_allowed_a)[0]  # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
    "regular_input_size = np.shape(regular_X)[0]  ## INPUT SIZE\n",
    "\n",
    "## INITIALIZE ENVIRONMENT WITH IMPROVED REWARDS\n",
    "improved_chess_environment = Chess_Env(BOARD_SIZE, R=-0.01, R_draw=-1.0, R_checked=1.0)\n",
    "improved_board, improved_X, improved_allowed_a = improved_chess_environment.Initialise_game()\n",
    "improved_n_possible_actions = np.shape(improved_allowed_a)[0]  # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
    "improved_input_size = np.shape(improved_X)[0]  ## INPUT SIZE\n",
    "\n",
    "## INITIALIZE ENVIRONMENT WITH MINIMAL INFORMATION\n",
    "minimal_chess_environment = Chess_Env(BOARD_SIZE, extended_features=False)\n",
    "minimal_board, minimal_X, minimal_allowed_a = minimal_chess_environment.Initialise_game()\n",
    "minimal_n_possible_actions = np.shape(minimal_allowed_a)[0]  # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
    "minimal_input_size = np.shape(minimal_X)[0]  ## INPUT SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc05bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRINT 5 STEPS OF AN EPISODE CONSIDERING A RANDOM AGENT\n",
    "\n",
    "def random_test(environment):\n",
    "    S, X, allowed_a = environment.Initialise_game()  # INTIALISE GAME\n",
    "\n",
    "    print(S)  # PRINT CHESS BOARD (SEE THE DESCRIPTION ABOVE)\n",
    "\n",
    "    print('check? ', environment.check)  # PRINT VARIABLE THAT TELLS IF ENEMY KING IS IN CHECK (1) OR NOT (0)\n",
    "    print('dofk2 ', np.sum(environment.dfk2_constrain).astype(int))  # PRINT THE NUMBER OF LOCATIONS THAT THE ENEMY KING CAN MOVE TO\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        a, _ = np.where(allowed_a == 1)  # FIND WHAT THE ALLOWED ACTIONS ARE\n",
    "        a_agent = np.random.permutation(a)[0]  # MAKE A RANDOM ACTION\n",
    "\n",
    "        S, X, allowed_a, R, Done = environment.OneStep(a_agent)  # UPDATE THE ENVIRONMENT\n",
    "\n",
    "        ## PRINT CHESS BOARD AND VARIABLES\n",
    "        print('')\n",
    "        print(S)\n",
    "        print(R, '', Done)\n",
    "        print('check? ', environment.check)\n",
    "        print('dofk2 ', np.sum(environment.dfk2_constrain).astype(int))\n",
    "\n",
    "        # TERMINATE THE EPISODE IF Done=True (DRAW OR CHECKMATE)\n",
    "        if Done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 2 0]]\n",
      "check?  0\n",
      "dofk2  1\n",
      "\n",
      "[[0 3 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 2]]\n",
      "0  0\n",
      "check?  0\n",
      "dofk2  1\n",
      "\n",
      "[[0 0 3 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 2 0 0]]\n",
      "0  0\n",
      "check?  0\n",
      "dofk2  2\n",
      "\n",
      "[[0 0 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 0]\n",
      " [0 2 1 0]]\n",
      "0  0\n",
      "check?  0\n",
      "dofk2  2\n",
      "\n",
      "[[0 0 3 0]\n",
      " [0 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 1 0]]\n",
      "0  0\n",
      "check?  0\n",
      "dofk2  1\n",
      "\n",
      "[[0 0 0 3]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 2 1 0]]\n",
      "0  0\n",
      "check?  0\n",
      "dofk2  2\n"
     ]
    }
   ],
   "source": [
    "random_test(environment=regular_chess_environment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc16cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game(environment):\n",
    "    # PERFORM N_episodes=1000 EPISODES MAKING RANDOM ACTIONS AND COMPUTE THE AVERAGE REWARD AND NUMBER OF MOVES\n",
    "\n",
    "    S, X, allowed_a = environment.Initialise_game()\n",
    "    N_episodes = 1_000\n",
    "\n",
    "    # VARIABLES WHERE TO SAVE THE FINAL REWARD IN AN EPISODE AND THE NUMBER OF MOVES\n",
    "    count = []\n",
    "    rewards = []\n",
    "\n",
    "    for n in range(N_episodes):\n",
    "\n",
    "        S, X, allowed_a = environment.Initialise_game()  # INITIALISE GAME\n",
    "        Done = 0  # SET Done=0 AT THE BEGINNING\n",
    "        i = 1  # COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE\n",
    "\n",
    "        # UNTIL THE EPISODE IS NOT OVER...(Done=0)\n",
    "        while Done == 0:\n",
    "\n",
    "            # SAME AS THE CELL BEFORE, BUT SAVING THE RESULTS WHEN THE EPISODE TERMINATES\n",
    "\n",
    "            a, _ = np.where(allowed_a == 1)\n",
    "            a_agent = np.random.permutation(a)[0]\n",
    "\n",
    "            S, X, allowed_a, R, Done = environment.OneStep(a_agent)\n",
    "\n",
    "            if Done:\n",
    "                count.append(R)\n",
    "                rewards.append(i)\n",
    "                break\n",
    "\n",
    "            i = i + 1  # UPDATE THE COUNTER\n",
    "    return count, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Agent, Average reward: 0.196 Number of steps:  7.297\n"
     ]
    }
   ],
   "source": [
    "# AS YOU SEE, THE PERFORMANCE OF A RANDOM AGENT ARE NOT GREAT, SINCE THE MAJORITY OF THE POSITIONS END WITH A DRAW\n",
    "# (THE ENEMY KING IS NOT IN CHECK AND CAN'T MOVE)\n",
    "random_count, random_rewards = random_game(environment=regular_chess_environment)\n",
    "print('Random_Agent, Average reward:', np.mean(random_count), 'Number of steps: ', np.mean(random_rewards))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "3829534d",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "The following is an implementation of a general **Neural Network Class** and a reinforcement learner using **SARSA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a306cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    W1, W2 = None, None\n",
    "\n",
    "    def __init__(self, hidden_layer, input_dim, output_dim, eta=0.02, rho=0.99, rmsprop=False):\n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(hidden_layer + 1, input_dim + 1) * 1.0 / np.sqrt(input_dim + 1)\n",
    "        self.W2 = np.random.randn(output_dim, hidden_layer + 1) * 1.0 / np.sqrt(hidden_layer + 1)\n",
    "        # Step size\n",
    "        self.eta = eta\n",
    "        # RMSprop parameters\n",
    "        self.rho = rho\n",
    "        self.V1 = np.zeros(self.W1.shape)\n",
    "        self.V2 = np.zeros(self.W2.shape)\n",
    "        self.l1 = None\n",
    "        self.l2 = None\n",
    "        self.rmsprop = rmsprop\n",
    "       \n",
    "    @staticmethod\n",
    "    def relu(A):\n",
    "        return np.maximum(0, A)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(X, T, Y, H, W2, Z1, Z2):\n",
    "        # Add bias term\n",
    "        X_bias = np.vstack((np.ones(X.shape[1]), X))\n",
    "\n",
    "        # d Loss / d Y\n",
    "        G_Y = 2. * (Y - T)\n",
    "        # d Y / d Z2\n",
    "        G_Z2 = G_Y * np.maximum(0, np.sign(Z2))\n",
    "        # d Z2 / d W2\n",
    "        G_W2 = np.dot(G_Z2, H.T)\n",
    "        # Layer 2 gradient\n",
    "        G2 = (1. / X_bias.shape[1]) * G_W2\n",
    "\n",
    "        # d Z2 / d H\n",
    "        G_H = np.dot(W2.T, G_Z2)\n",
    "        # d H / d Z1\n",
    "        G_Z1 = G_H * np.maximum(0, np.sign(Z1))\n",
    "        # d Z1 / d W1\n",
    "        G_W1 = np.dot(G_Z1, X_bias.T)\n",
    "        # Layer 1 gradient\n",
    "        G1 = (1. / X_bias.shape[1]) * G_W1\n",
    "        return G1, G2\n",
    "\n",
    "    def descent(self, X, T, H, Y, Z1, Z2):\n",
    "        G1, G2 = Network.gradient(X, T, Y, H, self.W2, Z1, Z2)\n",
    "\n",
    "        if self.rmsprop:\n",
    "            self.V1 = (self.rho * self.V1) + ((1.0 - self.rho) * np.square(G1))\n",
    "            print(G1, np.min(G1), np.max(G1))\n",
    "            self.V2 = (self.rho * self.V2) + ((1.0 - self.rho) * np.square(G2))\n",
    "\n",
    "            # print(np.dot(G1, (G1 / (np.sqrt(self.V1) + 1e-7)).T))\n",
    "            print(\"Under wurzel\", (np.sqrt(self.V1) + 1e-7))\n",
    "            print(\"Multi\", (G1 / (np.sqrt(self.V1) + 1e-7)))\n",
    "            print(\"reduction\", self.eta * (G1 / (np.sqrt(self.V1) + 1e-7)))\n",
    "            print(\"min w1, max w1\", np.min(self.W1), np.max(self.W1))\n",
    "            print(\"min w2, max w2\", np.min(self.W2), np.max(self.W2))\n",
    "\n",
    "            self.W1 = self.W1 - self.eta * (G1 / (np.sqrt(self.V1) + 1e-7))\n",
    "            self.W2 = self.W2 - self.eta * (G2 / (np.sqrt(self.V2) + 1e-7))\n",
    "        else:\n",
    "            self.W1 -= self.eta * G1\n",
    "            self.W2 -= self.eta * G2\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Add Bias to first layer input\n",
    "        X_bias = np.vstack((np.ones(X.shape[1]), X))\n",
    "        # First Layer\n",
    "        Z1 = np.dot(self.W1, X_bias)\n",
    "        H = Network.relu(Z1)\n",
    "        # Fix Bias for second layer input\n",
    "        H[0, :] = 1.\n",
    "        # Second Layer\n",
    "        Z2 = np.dot(self.W2, H)\n",
    "        Y = Network.relu(Z2)\n",
    "        return Y, H, Z1, Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea9bb9e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qvalues, epsilon):\n",
    "    N_class = np.shape(Qvalues)[0]\n",
    "    batch_size = np.shape(Qvalues)[1]\n",
    "\n",
    "    rand_values = np.random.uniform(0, 1, [batch_size])\n",
    "\n",
    "    rand_a = rand_values < epsilon\n",
    "    a = np.zeros([batch_size, N_class])\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        if rand_a[i]:\n",
    "            valid_moves = np.where(Qvalues[:,i] > -1000)[0]\n",
    "            chosen = np.random.choice(valid_moves)\n",
    "            a[i, chosen] = 1\n",
    "        else:\n",
    "            a[i, np.argmax(Qvalues[:, i])] = 1\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def test(network, environment, number_of_episodes):\n",
    "    nr_moves = []\n",
    "    total_rewards = []\n",
    "    stuck_runs = 0\n",
    "    for i in range(number_of_episodes):\n",
    "\t    done = 0\n",
    "\t    S, X, allowed_a = environment.Initialise_game()\n",
    "\t    X = X.reshape(len(X), 1)\n",
    "\t    total_reward = 0\n",
    "\t    count = 0\n",
    "\t    while done == 0:\n",
    "\t\t    count += 1\n",
    "\t\t    Q_values, _, _, _ = network.forward(X)\n",
    "\t\t    masked_Q_values = Q_values - (1 - allowed_a) * 100000\n",
    "\t\t    a_agent = epsilon_greedy_policy(masked_Q_values, 0).T\n",
    "\t\t    S, X, allowed_a, R, done = environment.OneStep(np.argmax(a_agent))\n",
    "\t\t    X = np.array(X).reshape(len(X),1)\n",
    "\t\t    total_reward += R\n",
    "\t\t    if count > 100:\n",
    "\t\t\t    stuck_runs += 1\n",
    "\t\t\t    break\n",
    "\t    nr_moves.append(count)\n",
    "\t    total_rewards.append(total_reward)\n",
    "    return nr_moves, total_rewards, stuck_runs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ba1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(environment, network, number_of_episodes, epsilon, B, y):\n",
    "    count = []\n",
    "    rewards = []\n",
    "\n",
    "    for n in range(number_of_episodes):\n",
    "\n",
    "        epsilon_f = epsilon / (1 + B * n)  ## DECAYING EPSILON\n",
    "        Done = 0  ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
    "        i = 1  ## COUNTER FOR NUMBER OF ACTIONS\n",
    "        total_reward = 0 ## COUNTER FOR TOTAL REWARD\n",
    "\n",
    "        S, X, allowed_a = environment.Initialise_game()  ## INITIALISE GAME\n",
    "        X = X.reshape(len(X), 1)\n",
    "\n",
    "        if n > 0 and n % 100 == 0:\n",
    "            print(f\"\\rEp.: {n}, epsilon: {epsilon_f:.3f}, moves: {np.mean(count[n - 100:]):.2f}\", end=\"\")\n",
    "\n",
    "        Q_values, H, Z1, Z2 = network.forward(X)\n",
    "        masked_Q_values = Q_values - (1 - allowed_a) * 100000\n",
    "        a_agent = epsilon_greedy_policy(masked_Q_values, epsilon_f).T\n",
    "\n",
    "        while Done == 0:  ## START THE EPISODE\n",
    "\n",
    "            S_next, X_next, allowed_a_next, R, Done = environment.OneStep(np.argmax(a_agent))\n",
    "            X_next = np.array(X_next).reshape(len(X_next), 1)\n",
    "            \n",
    "            total_reward += R\n",
    "\n",
    "            ## THE EPISODE HAS ENDED, UPDATE...BE CAREFUL, THIS IS THE LAST STEP OF THE EPISODE\n",
    "            if Done == 1:\n",
    "                output = Q_values * a_agent\n",
    "                target = R * a_agent\n",
    "                network.descent(X, target, H, output, Z1, Z2)\n",
    "                count.append(i)\n",
    "                rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "            # IF THE EPISODE IS NOT OVER...\n",
    "            else:\n",
    "                Q_values_next, H_next, Z1_next, Z2_next = network.forward(X_next)\n",
    "                masked_Q_values_next = Q_values_next - (1 - allowed_a_next) * 100000\n",
    "                a_agent_next = epsilon_greedy_policy(masked_Q_values_next, epsilon_f).T\n",
    "                future_R = Q_values_next[np.argmax(a_agent_next)]\n",
    "                output = Q_values * a_agent\n",
    "                target = (R + y * future_R) * a_agent\n",
    "                network.descent(X, target, H, output, Z1, Z2)\n",
    "\n",
    "            # NEXT STATE AND CO. BECOME ACTUAL STATE...     \n",
    "            S = np.copy(S_next)\n",
    "            X = np.copy(X_next)\n",
    "            allowed_a = np.copy(allowed_a_next)\n",
    "            Q_values = np.copy(Q_values_next)\n",
    "            H = np.copy(H_next)\n",
    "            a_agent = np.copy(a_agent_next)\n",
    "            Z1 = np.copy(Z1_next)\n",
    "            Z2 = np.copy(Z2_next)\n",
    "\n",
    "            i += 1  # UPDATE COUNTER FOR NUMBER OF ACTIONS\n",
    "    return count, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "network_sarsa = Network(hidden_layer=200, input_dim=regular_input_size, output_dim=regular_n_possible_actions, eta=0.0035)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43ece0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep.: 9900, epsilon: 0.134, moves: 3.49\n",
      "SARSA Agent training, Average reward: 0.7608 Number of steps:  4.7772\n",
      "\n",
      "SARSA Agent testing, Average reward: 0.9706 Number of steps:  3.0644 Nr of stucks:  4\n"
     ]
    }
   ],
   "source": [
    "sarsa_count, sarsa_rewards = sarsa(environment=regular_chess_environment, network=network_sarsa, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.2, B=0.00005, y=0.85)\n",
    "print('\\nSARSA Agent training, Average reward:', np.mean(sarsa_rewards), 'Number of steps: ', np.mean(sarsa_count))\n",
    "nr_moves_sarsa_test, rewards_sarsa_test, stucks_sarsa_test = test(network=network_sarsa, environment=regular_chess_environment, number_of_episodes=5_000)\n",
    "print('\\nSARSA Agent testing, Average reward:', np.mean(rewards_sarsa_test), 'Number of steps: ', np.mean(nr_moves_sarsa_test), 'Nr of stucks: ', stucks_sarsa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6fbfa2",
   "metadata": {},
   "source": [
    "### Plot 1: Reward per Game over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f5446f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ema_moves = pd.DataFrame(sarsa_rewards).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"SARSA Reward / Game\")\n",
    "plt.plot(ema_moves)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a48f9f",
   "metadata": {},
   "source": [
    "### Plot 2: Number of Moves per Game vs. training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b288ad57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEICAYAAACpqsStAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAiCElEQVR4nO3deZhcdZ3v8fe3lt6700s6obMTiJDASIIBAuqIAqI4is4dUUbHOFcfxuss6vUZLyjjiKOPjo8zMuNcF2ZcGFnEq6gMKItBZEQIBohhSQIxZF+6k+5Oeu+uqu/945zqVDXVC53uVJ3O5/U89VTV75w69f21+Mmpb506x9wdERGJplixCxARkclTiIuIRJhCXEQkwhTiIiIRphAXEYkwhbiISIQpxEVEIkwhLiISYQpxAcDMXmNmvzGzI2bWbmaPmNl5I9apNrNuM/tZgdfvMLO+cPkBM/uumdXkLF9gZj8ys0PhezxtZu8vsJ2HzKzDzMrHqfchM3MzO2fE+E/C8Ytf5p9gWpnZ/Wb2xlGWrTazu8N5d5rZc2b2eTNrONF1SvQoxAUzqwPuBr4KNALzgRuAgRGr/kk49kYzaymwqbe6ew2wElgFXJez7HvAbmAx0AS8Dzg4oo4lwGsBB942gdKfD7eTfX0TsAZom8BrTxgzqwZeBfyqwLKLgIeAR4Az3b0eeBOQAs4Zub7ISApxAXgFgLvf7u5pd+9z9/vdfdOI9dYC3wA2Ae8ZbWPufgC4jyDMs84DvuvuPe6ecven3P3nI176PuAx4Lvhe43nVuBdZhYPn18N/BgYzK5gZuVmdqOZ7QtvN2b38s1ss5n9Uc66ifCTwrnh8zXhp5NOM/td7t69mb3fzLabWZeZvWhmo/49gEuAR9x95D+KAF8CvuPuX3D3gwDuvsvd/97dHwrf6zQze9DMDof13Wpm9Tm17DCzvzWzTWbWY2bfMrO5ZvbzsL5f5O7VjzUviSB31+0kvwF1wGHgZuDNQEOBdRYBGWAF8HFg04jlO4BLw8cLgKeBf8lZ/guCvc13A4tGqWMb8GGCvdYhYO4YNT8EfBC4H3hzOPY4cCGwB7g4HPsswT8Mc4Bm4DfAP4TLPg3cmrPNtwBbwsfzw7/JFQQ7O5eFz5uBauAocEa4bgtw1hi1fgP4iwLj1UA6W+sYrz89fP/y8P0fBm4c8bd/DJgb1t0KPEnwaagceBD4+/HmVez/DnWb3K3oBehWGjdgOcEe8B6Cj/J35YYocD2wMXw8LwyfVTnLdwDdQBdBO2QdUJ+zvAH4IvBs+NqNwHk5y18TBvfs8PkW4GNj1JsN8fcCtwNnAM+Hy3JD/PfAFTmvuxzYET4+Pay3Knx+K/Dp8PH/Ab434j3vI/iEUA10Av8DqJzA33YnsLDA+ILwb3VmztiXwm33ANePsr23A0+N+Nu/J+f5j4Cv5zz/a+An482r2P8N6ja5m9opAoC7b3b397v7AuBsgqC+MWeV9xGEHO6+j6C/O7Ll8XZ3rwUuBs4EZudsv8Pdr3X3swj2GDcCPzEzC1dZC9zv7ofC57cV2H4hdwJvIAiq7xVYPo8gRLN2hmO4+zZgM/BWM6si6MPfFq63GHhn2HLoNLNOgn9oWty9B3gX8CFgv5ndY2ZnFirOzP4AOOruuwss7iD4dDP8/YK7f8KDvviPgUS4jTlm9n0z22tmR4FbyPnbhnK/X+gr8Dz7JfOo8ypUv5Q+hbi8hLtvIdgrPxuGv3xbBlwXHnlyALgAuNrMEgVe/6vw9V8eZfuHwmXzgEYzqwSuAl6Xs/2PAeeMPPqkwLZ6gZ8D/4vCIb6PILiyFoVjWbcT9NKvBJ4Lgx2CL2G/5+71Obdqd/9i+L73uftlBOG3Bfj3UUq8ArhnlNp7gPXAH481R+ALBHvsr3T3OoJPHzb2S0Y15rwkehTigpmdaWYfN7MF4fOFBMH2WLjKWuABgn74yvB2NlBF0EMv5EbgMjNbGW7zH83s7PDLw1qC0N3m7ocJ2gPpEdtfDvw3OUefjOGTwOvcfUeBZbcD15tZs5nNJuiD35Kz/PvAG8N6bssZv4VgD/1yM4ubWYWZXRweKjnXzN4WHnUyQNBGSo9S21uAlxySmeMTwP80s2vNbA4Eh2MCp+asUxu+R6eZzQf+doztjWfUeR3HNqWIFOICQV/4AmC9mfUQhPczwMfNrIJgL/mr7n4g5/YiwZ5vwZaHu7cB/wn8XThURdAi6AS2E+wdZw8jXEtwhMau3PcA/g14T6G9/RHvtc/dfz3K4s8BGwiOqHma4Au/z+W8dj/wKHARcEfO+G6CvfNPEhyyuJsgPGPh7eMEe/TtwOsIvpDNY2azCP4x+s0Ytf+aoB30h8DzYXvjXoKe/1fD1W4AzgWOEOzV3zna9sYzzrwkgsxdV/YRmQ5mdhXwJ+5+VbFrkZlL//qKTJ9O4CvFLkJmNu2Ji4hEmPbERUQibMwvjKba7NmzfcmSJSfyLUVEIu+JJ5445O7NhZad0BBfsmQJGzZsOJFvKSISeWa2c7RlaqeIiESYQlxEJMIU4iIiEaYQFxGJMIW4iEiEKcRFRCJMIS4iEmGRCPF0xvnBht2kMzpFgIhIrkiE+K3rd/KJH27iPx/dUexSRERKSiRCvL0nuHh5R8/gOGuKiJxcIhHiFl6JSs0UEZF80QjxyV5NUERkhotEiGfp1OciIvkiEeLZHXFXQ0VEJE80QlztFBGRgiIR4llqp4iI5ItEiJvp6BQRkULGDXEzqzCzx83sd2b2rJndEI43mtkDZvZCeN8w/eWKiEiuieyJDwBvcPdzgJXAm8xsDXAtsM7dlwHrwufTSu0UEZF844a4B7rDp8nw5sCVwM3h+M3A26ejQDj2xaaOThERyTehnriZxc1sI9AKPODu64G57r4fILyfM8prrzGzDWa2oa2tbVJFGjo8RUSkkAmFuLun3X0lsAA438zOnugbuPtN7r7a3Vc3NzdPsszsxo7v5SIiM83LOjrF3TuBh4A3AQfNrAUgvG+d6uKyjrVTREQk10SOTmk2s/rwcSVwKbAFuAtYG662FvjpNNWoZoqIyCgSE1inBbjZzOIEof8Dd7/bzB4FfmBmHwB2Ae+cxjoBcB2eIiKSZ9wQd/dNwKoC44eBS6ajqJGG2ynKcBGRPNH4xaYaKiIiBUUixLO0Iy4iki8SIa52iohIYZEIcRERKSxSIa6f3YuI5ItEiA+filYZLiKSJxohXuwCRERKVDRCfPiLTe2Ki4jkikaIh/eKcBGRfNEIcV0pWUSkoEiEeJa6KSIi+SIR4rqyj4hIYdEI8WIXICJSoiIR4llqp4iI5ItGiGd/7FPkMkRESk0kQlztFBGRwiIR4llqp4iI5ItEiB87TFwpLiKSKxohroaKiEhBkQjxLLVTRETyRSLEdWUfEZHCohHixS5ARKRERSLEs/SzexGRfJEIcbVTREQKi0aIq6EiIlJQJEI8SzviIiL5xg1xM1toZr80s81m9qyZfSQc/4yZ7TWzjeHtimmrUu0UEZGCEhNYJwV83N2fNLNa4AkzeyBc9hV3//L0lRdQM0VEpLBxQ9zd9wP7w8ddZrYZmD/dhRWsRQ0VEZE8L6snbmZLgFXA+nDor8xsk5l928waRnnNNWa2wcw2tLW1TapIO3ZpHxERyTHhEDezGuBHwEfd/SjwdeA0YCXBnvo/FXqdu9/k7qvdfXVzc/OkilQ7RUSksAmFuJklCQL8Vne/E8DdD7p72t0zwL8D509fmQHtiIuI5JvI0SkGfAvY7O7/nDPekrPaO4Bnpr687HsF967DU0RE8kzk6JRXA38GPG1mG8OxTwJXm9lKgh3kHcBfTEN9QO75xEVEJNdEjk75NYXb0j+b+nLGqeVEv6GISImLxC82dx7uBfRjHxGRkSIR4t/+9YvFLkFEpCRFIsSztCMuIpIvEiEeiwUteR2dIiKSLxIhroNTREQKi0SIZ2k/XEQkXyRCPKZzp4iIFBSJENePfURECotEiGfpVLQiIvkiEeLZU9FmMkUuRESkxEQixLMyOsRQRCSPQlxEJMIiFeLpjEJcRCRXpEJcGS4iki9iIa4UFxHJpRAXEYmwaIW4DjEUEckTqRBPa09cRCRPpEI8o282RUTyRCvEtScuIpInUiGeVoaLiOSJVIirnSIiki9aIa52iohInoiFeLErEBEpLdEKcaW4iEieaIW42ikiInnGDXEzW2hmvzSzzWb2rJl9JBxvNLMHzOyF8L5huovVj31ERPJNZE88BXzc3ZcDa4C/NLMVwLXAOndfBqwLn08rtVNERPKNG+Luvt/dnwwfdwGbgfnAlcDN4Wo3A2+fphqHKcNFRPK9rJ64mS0BVgHrgbnuvh+CoAfmTHl1I6gnLiKSb8IhbmY1wI+Aj7r70ZfxumvMbIOZbWhra5tMjcPUThERyTehEDezJEGA3+rud4bDB82sJVzeArQWeq273+Tuq919dXNz83EVqwwXEck3kaNTDPgWsNnd/zln0V3A2vDxWuCnU19ePh2dIiKSLzGBdV4N/BnwtJltDMc+CXwR+IGZfQDYBbxzWioELLxXO0VEJN+4Ie7uv+ZYjo50ydSWM0oN4b2+2BQRyRexX2wWuwIRkdISiRBXO0VEpLBIhLjaKSIihUUixLN6BtPFLkFEpKREIsRH+1ZVRORkF4kQVxNFRKSwSIS4iIgUFokQVztFRKSwSIR4bjvFdYSKiMiwSIR4Lh0qLiJyTCRCPLedklaKi4gMi0SI58a2QlxE5JhIhHiuVCZT7BJEREpGJEJc7RQRkcIiEeJqp4iIFBaJEM+lEBcROSYSIZ7bTkkpxEVEhkUixNVOEREpLBIhnkshLiJyTCRCXO0UEZHCIhHiubE9lNZx4iIiWZEI8Vy3rd9V7BJEREpGJEI8t52yvKWuaHWIiJSaSIR4bjulvipZtDpEREpNJEI814dvfbLYJYiIlIxIhLiu7CMiUlgkQlwHFYqIFDZuiJvZt82s1cyeyRn7jJntNbON4e2K6S1TREQKmcie+HeBNxUY/4q7rwxvP5vasvKpnSIiUti4Ie7uDwPtJ6CW0Wso5puLiJSw4+mJ/5WZbQrbLQ2jrWRm15jZBjPb0NbWdhxvF7jg1Mbj3oaIyEwx2RD/OnAasBLYD/zTaCu6+03uvtrdVzc3N0/qzbLtlIaqpI4TFxHJMakQd/eD7p529wzw78D5U1vWiPcL78sTcQZSOneKiEjWpELczFpynr4DeGa0dadSWSLGwJBCXEQkKzHeCmZ2O3AxMNvM9gB/D1xsZisJdpJ3AH8xfSUea6eUJ2IM6iyGIiLDxg1xd7+6wPC3pqGW0WsI78uTMQZS6RP51iIiJS0Sv9jMKournSIikisSIX6snRJXO0VEJEckQjyvnaI9cRGRYZEI8ayyuHriIiK5IhHiw+2UZJxBHScuIjIsEiGeFeyJK8RFRLIiFeLlyRipjJPO6JRYIiIQtRBPBOWqpSIiEohUiJeFId4/pC83RUQgYiH+zV9tB+BrD20rciUiIqUhUiH+Z2sWA9BcW17kSkRESkOkQvyt58wDoLFaIS4iAhEL8brK4HxdR/uGilyJiEhpiFSI11YEV/X57N3PFbkSEZHSEKkQT8R03XsRkVyRCnEREckXuRB/2znzWNJUVewyRERKQiRCPPdH9k/s7GDH4V72dPQWrR4RkVIRjRD3IMbNYG9nHwBP7zlSzJJEREpCJEI8e76ruBlXrV4AwJBOgiUiEo0QX7WwHgjOJ/5Xr18GwN/c/pROhCUiJ71xr3ZfCr76p6vY3tZDTXmC6rL48PhPNu7lqtULi1iZiEhxRWJPvKoswdnzZwFgduxY8YNH+otVkohISYhEiI9m4+7OYpcgIlJUkQzxtRcGZzNct6WVzt7BIlcjIlI8kQzxG648e/jxys8+UMRKRESKa9wQN7Nvm1mrmT2TM9ZoZg+Y2QvhfcP0lvlSsyqTJ/otRURKzkT2xL8LvGnE2LXAOndfBqwLn59QS2ZXDz/O6JhxETlJjRvi7v4w0D5i+Erg5vDxzcDbp7as8b3ngkXDj5d+8mcn+u1FRErCZHvic919P0B4P2e0Fc3sGjPbYGYb2traJvl2L3XV6oX8y7tXDj/feqBryrYtIhIV0/7Fprvf5O6r3X11c3PzlG579ZLG4ceX3/jwlG5bRCQKJhviB82sBSC8b526kiZufn0l179l+fBz9cZF5GQz2RC/C1gbPl4L/HRqynn5PvjapdRWBGcPWPrJn/H9x3cVqxQRkRNuIocY3g48CpxhZnvM7APAF4HLzOwF4LLwedF87T3nDj++9s6ni1iJiMiJNe4JsNz96lEWXTLFtUzaa5fl99rbugbY3tbNBUubJryNvZ19zK+vnOrSRESmVSR/sVnI599x7Fec533+F7zrpsf4+dP7h8cyGeeRbYc42j80PDaUzpBKZ7j98V28+osPctt6tWJEJFose9WcE2H16tW+YcOGadt+R88gq/4h/2f4p8+p4YGP/SGvuP7nDKXHn+v3r1nDmpexBy8iMt3M7Al3X11o2YzZEwdoqC7jiesvpWVWxfDYttZuTr3uZxMKcIB33/QYH7z5t9NVoojIlJpRIQ7QVFPOo9ddwvOfezMfuWTZS5bf8oELeF94FkSAh//29QB898/PY2V4BaFfbG5lybX38OjvD7Pk2nu44b+eJZXWVYREpPTMqHZKIeu3H+ZdNz3G+y9awqfespxkfOx/t1q7+jn/8+sKLnv6M2+ktkIn3hKRE2usdsqMD/HJcHcu+uKD7D/Sz+VnzeW+Zw/mLX/0ujfQMktHsojIiaEQP06ZjHPu5x6gs/fYkS3Xv2U5H3zt0iJWJSInC4X4FBlMZfjwrU/wi83HzjKgvXIRmW4nzdEp060sEeM/1p7HHdes4ZULggs3X/iFB1ly7T3c8VsdYy4iJ572xI/Db3e0885vPDr8vCIZ48m/u4yqsnF/CCsiMmHaE58m5y1p5N6PvpY7rlnDH72yhf6hDOd/fh1P7eoodmkicpLQLuNxOvOUOgAuWNrEpcv38tE7NvKOr/2Gv7lkGWfNq6N3MMVX123jI5cu48qV84tcrYjMNGqnTLHHth/mL299ksM9gy9ZNqe2nNauAQC+8+fn8fozRr0gkojIMB2dcoKlM87n79nMtx95kbJEjMFU4V97njWvjqtWL2TFvDpeMbeW8kSMimQcgK89tI3lLXVc/IpmzOxEli8iJUYhXiK2t3XzoVue4BOXn8kPn9jDvc8eGPc1i5uqWDq7msF0hsvPOoWrz1807q9ORWRmUYiXqANH+vnSvVu486m9BZdfcuYcNuzs4Ejf0EuWXX3+Ij566TLm1lUUeKWIzCQK8YhJZ5yYgZmRSmf49F3PUleRZOfhHjbtOcLezr689c9dVM+bz27hTy9YRHW5vqsWmWkU4jPQdx55kRv+67m8sfn1lXz6rStYNqeGpppyZlXqZF0iM4FCfIZr7ernwc2t/N+HtrG7/dhe+qXL5/Kptyzn1NnVRaxORI6XQvwk0TeY5r5nD/CjJ/fw3y8cGh5Pxo1Xnz6buBmza8p575rF/EF42gARKX0K8ZNQOuNs3n+UWx7byVO7Otl6sCtv+azKJDXlCT70uqX88bkLqEzGicV0KKNIKVKIC/1Dae5/7iAvtvXwfGsXuw738vTeIy9Z7x2r5nPvMwdY1FjFh19/Guef2qizNIoUmUJcCnJ3fvP7w6zffpj7nzvIlgNdBdcri8c4Z+Es/mB+PZcun8N5pzbqWHWRE0ghLhOWzjipTIZU2nlk2yH+6f7n6egdHD5dAMC8WRV84LVL+dPzF1FZFi9itSInB4W4HDd3p71nkHufPcBPntrLb3d0UJaIMae2nHjMePvK+dRWJHh231He+aoFzK4tpzIZ55RZFdprFzlOCnGZUu7O4y+288BzB3l8Rzub9ry0t55VVRZnRUsdi5qquGz5XC5dMVehLvIyTVuIm9kOoAtIA6nR3iRLIT5z9Q+l2dXeywsHu6mtSNDVn+JI3xCb9x9l8/6jbNh57BzrCxsruXzFKaxZ2sQZp9SysLGqiJWLlL7pDvHV7n5ovHVBIX4yO9Q9wCPbDnH/cwe5Z9P+vGXliRipjLN6cQMVyThliRiLG6to7xlkfkMlc+sqaD3aTyIeo38ozelzavjyfVuprUhSU5FgeUst6YyzuKmai05rYtmcWvXqZUZRiEvJ6RlI8dDWNrYeOMqDW1vZeaiXusok7s6+I/0kYkYqc3ytvlctbqB3MM0pdeU0VpfTXFvORac1cf6pjcOn/BWJgukM8ReBDsCBb7r7TQXWuQa4BmDRokWv2rlz56TfT04emYzjwFA6w6HuAQ4e7WdefSXd/Sn2dPZRX5lk5cJ60mHQ9w6l6RtM88TODp4/2MUvt7TSP5RhZ3sPS5qqaesaGL5QRzJuzKoso74qScusCpa31LF6cQPnn9pIfVUZAAOpNGXxmM7lLiVhOkN8nrvvM7M5wAPAX7v7w6Otrz1xKabewRSPbDvMf7/QxuHuQQbTGQ4c6Wfz/qPDe/0xg2Q8xkAqQ8usCla01LFsbi1lceOp3Z3UVSY5a14dZ8+bxfyGSpY0VRPXL11lmp2Qo1PM7DNAt7t/ebR1FOJSinoHU/xu9xGe3NXBwaP9pDJOfWWSne29bDvYzfZD3Qylg/+fzK+vzDsVcPbom/kNlSybU8OS2dWcMbeWxU3VlCV0FI5MjbFCfNInnzazaiDm7l3h4zcCn53s9kSKpaoswYWnNXHhaU0Flw+lM+xu72VBQxVliRiHugd44WA3uzt6eW7fUZ7Ze4T129v56cZ9w69Jxo2FDVWcPqeGpc01zK0rZ9OeI6xaVM+syiTliThVZXEaq8tY3FRFbYVOGyyTczxXEJgL/DjsGSaA29z93impSqSEJOMxljbXDD+fXVPO7JpyLiQ/9HsHU+w41Mtz+4/y/MEudrf3svVAF+u2tA737n88ylWcsttdNqeGqrI4TTVlVCbjzG+opDIZp7G6nIaqJD2DaRY3VdEyq4LqsoROWiaTD3F33w6cM4W1iERaVVmCFfPqWDGvLm/c3TkU9uCTMeNwzyD9Q2n6htLs7eijtWuA37d18/DzbQymM7R3DPLkrg76htL0DxW+yDZAImY0VpcNB30iHqMsHqOyLM7smjIWNlSRCD8RVJbFmVNbTk15gtqKpA7BnEF0LS+RaWZmNNeWDz+fM8Hroro7XQMp+ofSdPQM0d4zyL7OPpzgQiC9A2lau/rZ09FH/1Caof4Ug6kMfeEPr8bSMquCWZVJ6iqTLGiopK4iycLGKhqrk1Qm49RVJplbV8H2th6qy+PMr6/EMCqSMdLupNLOoe4BkvEYCxoqmVWZ1JE8RaIQFylRZkZdRZK6iiRzal/eBbEzGaejd5ChtLPvSB+723tJpZ2+oTStXQPsbu+leyDF4e4BfrW1jf6hND2D6eOqd25dORXJOBWJ4Nz0dRUJ6quS1FeW0VRTxkAqw/a2buqrymjt6mdXey/zZlWyvKWOV8ytZVFjFVXlwetn15aRiMUoT8SoLk8wkEqTyUAibsTNMIOewTSJmFGeKHwo6FA6gwHxmJHOOIkZeroHhbjIDBSLGU01wd7/KbMqOHdRw5jruzsdvUN09g7SP5ThcM8Au9v7aKmvIBEz9h/pZyidwT1o4yTiMY72DVFVFqezb4jf7e4kFgbpQCrN4Z5B9nT00dE7SGdvJ+09g8OHcS5oqKS9Z5DewSCYN+7uZCA1ettoPGbgHlzo5JS6CubUlXO0P8XvdncCwZfM6YxTX1VGTXmCusrgtBAGHOkboro8QXVZglmVSWIxaKopp6m6jL7BNLNry2muKWd+QyXPH+gi45BxZ3b4ySoZM+oqk8TMqCqL84q5tdRUJEjGjWQsNvydRd9getpaWApxEcEs6K83VpdNy/bdnYFUpuAvZd2dXe29HDw6QM9giv7BNAeP9tM7FHwyOHCkn32dfaxa1EAm42Qc+obSbGvt5qx5dQymM+zp6GPX4R5m15QHn0BSGc48pZZLl89lKJOhLB6jo3eQo30p2roGWNRYRTwWo7YiQe9Aiu6BNO09A8Tc2LzvKIe6ByhPxmnvGRz+UnoyKpIxqssSdA2k+OGHLuSVC+onva3RKMRFZNqZ2ainOjAzFjdVs7ip9C7onUpnONI3xO6OPpJx4/Q5NaTSTm/YehpMZ+jqHyKVdjp7h3jxUDc9g2lS6QyDaWdgKE33QIqMB78pmA4KcRGRUSTisaC9UnPsi+nyBFSX50bnscsXvmbZ7BNYXWBmdvpFRE4SCnERkQhTiIuIRJhCXEQkwhTiIiIRphAXEYkwhbiISIQpxEVEImzKruwzoTczawMme5HN2cCELsg8g2jOJwfN+eRwPHNe7O7NhRac0BA/Hma2YbTLE81UmvPJQXM+OUzXnNVOERGJMIW4iEiERSnEbyp2AUWgOZ8cNOeTw7TMOTI9cREReako7YmLiMgICnERkQiLRIib2ZvMbKuZbTOza4tdz2SZ2UIz+6WZbTazZ83sI+F4o5k9YGYvhPcNOa+5Lpz3VjO7PGf8VWb2dLjsX63ELzVuZnEze8rM7g6fz+g5m1m9mf3QzLaE/3tfeBLM+WPhf9fPmNntZlYx0+ZsZt82s1YzeyZnbMrmaGblZnZHOL7ezJaMW5S7l/QNiAO/B5YCZcDvgBXFrmuSc2kBzg0f1wLPAyuALwHXhuPXAv8YPl4RzrccODX8O8TDZY8DFwIG/Bx4c7HnN87c/zdwG3B3+HxGzxm4Gfhg+LgMqJ/JcwbmAy8CleHzHwDvn2lzBv4QOBd4JmdsyuYIfBj4Rvj43cAd49ZU7D/KBP5oFwL35Ty/Driu2HVN0dx+ClwGbAVawrEWYGuhuQL3hX+PFmBLzvjVwDeLPZ8x5rkAWAe8gWMhPmPnDNSFgWYjxmfynOcDu4FGgss+3g28cSbOGVgyIsSnbI7ZdcLHCYJfeNpY9UShnZL9jyNrTzgWaeHHpFXAemCuu+8HCO/nhKuNNvf54eOR46XqRuATQCZnbCbPeSnQBnwnbCH9h5lVM4Pn7O57gS8Du4D9wBF3v58ZPOccUznH4de4ewo4AjSN9eZRCPFC/bBIHxdpZjXAj4CPuvvRsVYtMOZjjJccM/sjoNXdn5joSwqMRWrOBHtQ5wJfd/dVQA/Bx+zRRH7OYR/4SoK2wTyg2szeO9ZLCoxFas4TMJk5vuz5RyHE9wALc54vAPYVqZbjZmZJggC/1d3vDIcPmllLuLwFaA3HR5v7nvDxyPFS9GrgbWa2A/g+8AYzu4WZPec9wB53Xx8+/yFBqM/kOV8KvOjube4+BNwJXMTMnnPWVM5x+DVmlgBmAe1jvXkUQvy3wDIzO9XMygia/XcVuaZJCb+B/haw2d3/OWfRXcDa8PFagl55dvzd4TfWpwLLgMfDj2xdZrYm3Ob7cl5TUtz9Ondf4O5LCP63e9Dd38vMnvMBYLeZnREOXQI8xwyeM0EbZY2ZVYW1XgJsZmbPOWsq55i7rT8h+P/L2J9Eiv0lwQS/SLiC4EiO3wOfKnY9xzGP1xB8NNoEbAxvVxD0vNYBL4T3jTmv+VQ4763kfEsPrAaeCZf9G+N8+VEKN+Bijn2xOaPnDKwENoT/W/8EaDgJ5nwDsCWs93sER2XMqDkDtxP0/IcI9po/MJVzBCqA/wdsIziCZel4Neln9yIiERaFdoqIiIxCIS4iEmEKcRGRCFOIi4hEmEJcRCTCFOIiIhGmEBcRibD/D4XatyMvc12gAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ema_moves = pd.DataFrame(sarsa_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"SARSA Moves / Game\")\n",
    "plt.plot(ema_moves)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f2bda2",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "We changed increased **epsilon_0 to 0.4**, decreased **gamma to 0.7**, decreased **beta to 0.0001**, increased **eta to 0.02** and increased the number of **hidden layers to 256**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37280e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "network_adapted_sarsa = Network(hidden_layer=256, input_dim=regular_input_size, output_dim=regular_n_possible_actions, eta=0.02 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29f8edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep.: 9900, epsilon: 0.201, moves: 3.24\n",
      "Adapted SARSA Agent training, Average reward: 0.6155 Number of steps:  5.2612\n",
      "\n",
      "Adapted SARSA Agent testing, Average reward: 0.9508 Number of steps:  3.4714 Nr of stucks:  48\n"
     ]
    }
   ],
   "source": [
    "adapted_sarsa_count, adapted_sarsa_rewards = sarsa(environment=regular_chess_environment, network=network_adapted_sarsa, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.4, B=0.0001, y=0.7)\n",
    "print('\\nAdapted SARSA Agent training, Average reward:', np.mean(adapted_sarsa_rewards), 'Number of steps: ', np.mean(adapted_sarsa_count))\n",
    "nr_moves_adapted_sarsa_test, rewards_adapted_sarsa_test, stucks_adapted_sarsa_test = test(network=network_adapted_sarsa, environment=regular_chess_environment, number_of_episodes=5_000)\n",
    "print('\\nAdapted SARSA Agent testing, Average reward:', np.mean(rewards_adapted_sarsa_test), 'Number of steps: ', np.mean(nr_moves_adapted_sarsa_test), 'Nr of stucks: ', stucks_adapted_sarsa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "344afaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ema_moves = pd.DataFrame(adapted_sarsa_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Adapted SARSA Moves / Game\")\n",
    "plt.plot(ema_moves)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a983c3c",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "We implemented **Q-Learning** and **Experience Replay** as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ed177",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7111f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearn(environment, network, number_of_episodes, epsilon, B, y):\n",
    "    count = []\n",
    "    rewards = []\n",
    "    \n",
    "    for n in range(number_of_episodes):\n",
    "\n",
    "        epsilon_f = epsilon / (1 + B * n)  ## DECAYING EPSILON\n",
    "        Done = 0  ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
    "        i = 1  ## COUNTER FOR NUMBER OF ACTIONS\n",
    "        total_reward = 0 ## COUNTER FOR TOTAL REWARD\n",
    "\n",
    "        S, X, allowed_a = environment.Initialise_game()  ## INITIALISE GAME\n",
    "        X = X.reshape(len(X), 1)\n",
    "\n",
    "        if n > 0 and n % 100 == 0:\n",
    "            print(f\"\\rEp.: {n}, epsilon: {epsilon_f:.3f}, moves: {np.mean(count[n - 100:]):.2f}\", end=\"\")\n",
    "\n",
    "        Q_values, H, Z1, Z2 = network.forward(X)\n",
    "        masked_Q_values = Q_values - (1 - allowed_a) * 100_000\n",
    "        a_agent = epsilon_greedy_policy(masked_Q_values, epsilon_f).T\n",
    "\n",
    "        while Done == 0:  ## START THE EPISODE\n",
    "\n",
    "            S_next, X_next, allowed_a_next, R, Done = environment.OneStep(np.argmax(a_agent))\n",
    "            X_next = np.array(X_next).reshape(len(X_next), 1)\n",
    "            total_reward += R\n",
    "\n",
    "            ## THE EPISODE HAS ENDED, UPDATE...BE CAREFUL, THIS IS THE LAST STEP OF THE EPISODE\n",
    "            if Done == 1:\n",
    "                output = Q_values * a_agent\n",
    "                target = R * a_agent\n",
    "                network.descent(X, target, H, output, Z1, Z2)\n",
    "                count.append(i)\n",
    "                rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "            # IF THE EPISODE IS NOT OVER...\n",
    "            else:\n",
    "                Q_values_next, H_next, Z1_next, Z2_next = network.forward(X_next)\n",
    "                masked_Q_values_next = Q_values_next - (1 - allowed_a_next) * 100_000\n",
    "                # Q-Learning chooses the next step greedily (epsilon=0)\n",
    "                a_agent_next = epsilon_greedy_policy(masked_Q_values_next, 0.0).T\n",
    "                future_R = Q_values_next[np.argmax(a_agent_next)]\n",
    "                output = Q_values * a_agent\n",
    "                target = (R + y * future_R) * a_agent\n",
    "                network.descent(X, target, H, output, Z1, Z2)\n",
    "\n",
    "            # NEXT STATE AND CO. BECOME ACTUAL STATE...     \n",
    "            S = np.copy(S_next)\n",
    "            X = np.copy(X_next)\n",
    "            allowed_a = np.copy(allowed_a_next)\n",
    "            Q_values = np.copy(Q_values_next)\n",
    "            H = np.copy(H_next)\n",
    "            Z1 = np.copy(Z1_next)\n",
    "            Z2 = np.copy(Z2_next)\n",
    "            # Q-Learning chooses next action based on greedy policty\n",
    "            a_agent = epsilon_greedy_policy(masked_Q_values_next, epsilon_f).T\n",
    "\n",
    "            i += 1  # UPDATE COUNTER FOR NUMBER OF ACTIONS\n",
    "    return count, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "network_qlearn = Network(hidden_layer=256, input_dim=regular_input_size, output_dim=regular_n_possible_actions, eta=0.02)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbc5710d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep.: 4100, epsilon: 0.284, moves: 3.38"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10380/4244087994.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mqlearn_count\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mqlearn_rewards\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mqlearn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menvironment\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mregular_chess_environment\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnetwork\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnetwork_qlearn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnumber_of_episodes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mNUMBER_OF_EPISODES\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepsilon\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mB\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.0001\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.7\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'\\nAdapted Q-Learning Agent, Average reward:'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mqlearn_rewards\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Number of steps: '\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mqlearn_count\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mnr_moves_q_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrewards_q_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstucks_q_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnetwork\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnetwork_qlearn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0menvironment\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mregular_chess_environment\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnumber_of_episodes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m5_000\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'\\nAdapted Q-Learning Agent testing, Average reward:'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrewards_q_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Number of steps: '\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnr_moves_q_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Nr of stucks: '\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstucks_q_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10380/2640989201.py\u001B[0m in \u001B[0;36mqlearn\u001B[1;34m(environment, network, number_of_episodes, epsilon, B, y)\u001B[0m\n\u001B[0;32m     37\u001B[0m             \u001B[1;31m# IF THE EPISODE IS NOT OVER...\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 39\u001B[1;33m                 \u001B[0mQ_values_next\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mH_next\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mZ1_next\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mZ2_next\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnetwork\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_next\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     40\u001B[0m                 \u001B[0mmasked_Q_values_next\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mQ_values_next\u001B[0m \u001B[1;33m-\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mallowed_a_next\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;36m100_000\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m                 \u001B[1;31m# Q-Learning chooses the next step greedily (epsilon=0)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10380/1639626028.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[0mX_bias\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mones\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m         \u001B[1;31m# First Layer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 66\u001B[1;33m         \u001B[0mZ1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mW1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_bias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     67\u001B[0m         \u001B[0mH\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mNetwork\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mZ1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m         \u001B[1;31m# Fix Bias for second layer input\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "qlearn_count, qlearn_rewards = qlearn(environment=regular_chess_environment, network=network_qlearn, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.4, B=0.0001, y=0.7)\n",
    "print('\\nAdapted Q-Learning Agent, Average reward:', np.mean(qlearn_rewards), 'Number of steps: ', np.mean(qlearn_count))\n",
    "nr_moves_q_test, rewards_q_test, stucks_q_test = test(network=network_qlearn, environment=regular_chess_environment, number_of_episodes=5_000)\n",
    "print('\\nAdapted Q-Learning Agent testing, Average reward:', np.mean(rewards_q_test), 'Number of steps: ', np.mean(nr_moves_q_test), 'Nr of stucks: ', stucks_q_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_moves = pd.DataFrame(qlearn_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Adapted Q-Learning Moves / Game\")\n",
    "plt.plot(ema_moves)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a35fb",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ee3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience:\n",
    "    def __init__(self, state_before, state_after, action, reward):\n",
    "        self.state_before = state_before\n",
    "        self.state_after = state_after\n",
    "        self.action = action\n",
    "        self.reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(environment, network, number_of_episodes, epsilon, B, y, batch_size=50, relevant_histories=300):\n",
    "    count = []\n",
    "    rewards = []\n",
    "    \n",
    "    experiences = []\n",
    "    \n",
    "    for n in range(int(number_of_episodes/5)):\n",
    "\n",
    "        epsilon_f = epsilon / (1 + B * n)  ## DECAYING EPSILON\n",
    "\n",
    "        Done = 0  ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
    "        i = 1  ## COUNTER FOR NUMBER OF ACTIONS\n",
    "        total_reward = 0 ## COUNTER FOR TOTAL REWARD\n",
    "\n",
    "        S, X, allowed_a = environment.Initialise_game()  ## INITIALISE GAME\n",
    "        X = X.reshape(len(X), 1)\n",
    "\n",
    "        if n > 0 and n % 100 == 0:\n",
    "            print(f\"\\rEp.: {n}, epsilon: {epsilon_f:.3f}, moves: {np.mean(count[n - 100:]):.2f}\", end=\"\")\n",
    "\n",
    "        while Done == 0:  ## START THE EPISODE\n",
    "            \n",
    "            Q_values, H, Z1, Z2 = network.forward(X)\n",
    "            masked_Q_values = Q_values - (1 - allowed_a) * 100_000\n",
    "            a_agent = epsilon_greedy_policy(masked_Q_values, epsilon_f).T\n",
    "\n",
    "            S_next, X_next, allowed_a_next, R, Done = regular_chess_environment.OneStep(np.argmax(a_agent))\n",
    "            X_next = np.array(X_next).reshape(len(X_next), 1)\n",
    "            total_reward += R\n",
    "            \n",
    "            if X_next.shape[0] == 0:\n",
    "                expi = Experience(X, X, a_agent, R)\n",
    "            else:\n",
    "                expi = Experience(X, X_next, a_agent, R)\n",
    "            experiences.append(expi)\n",
    "            \n",
    "            if len(experiences) > relevant_histories:\n",
    "                experiences = experiences[1:]\n",
    "                training_set = np.random.choice(experiences, batch_size, False)\n",
    "                \n",
    "                state_before = np.hstack([experience.state_before for experience in training_set])\n",
    "                Q_values_before, H_before, Z1_before, Z2_before = network.forward(state_before)\n",
    "                \n",
    "                actions = np.hstack([experience.action for experience in training_set])\n",
    "                Q_values_played = Q_values_before * actions\n",
    "                \n",
    "                states_after = np.hstack([experience.state_after for experience in training_set])\n",
    "                Q_values_after, _, _, _ = network.forward(states_after)\n",
    "\n",
    "                Q_values_after_played = np.max(Q_values_after, 0)\n",
    "                \n",
    "                rewards_played = np.hstack([experience.reward for experience in training_set])\n",
    "                \n",
    "                target = (rewards_played + y * Q_values_after_played) * actions\n",
    "                network.descent(state_before, target, H_before, Q_values_played, Z1_before, Z2_before)\n",
    "                \n",
    "            ## THE EPISODE HAS ENDED, UPDATE...BE CAREFUL, THIS IS THE LAST STEP OF THE EPISODE\n",
    "            if Done == 1:\n",
    "                count.append(i)\n",
    "                rewards.append(total_reward)\n",
    "                break\n",
    "                \n",
    "            allowed_a = np.copy(allowed_a_next)\n",
    "            X = np.copy(X_next)\n",
    "\n",
    "            i += 1  # UPDATE COUNTER FOR NUMBER OF ACTIONS\n",
    "    return count, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "network_replay = Network(hidden_layer=256, input_dim=regular_input_size, output_dim=regular_n_possible_actions, eta=0.02)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f171a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_count, replay_rewards = replay(environment=regular_chess_environment, network=network_replay, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.4, B=0.0001, y=0.7)\n",
    "print('\\nExperience Replay Agent training, Average reward:', np.mean(replay_rewards), 'Number of steps: ', np.mean(replay_count))\n",
    "nr_moves_er_test, rewards_er_test, stucks_er_test = test(network=network_replay, environment=regular_chess_environment, number_of_episodes=5_000)\n",
    "print('\\nExperience Replay Agent testing, Average reward:', np.mean(rewards_er_test), 'Number of steps: ', np.mean(nr_moves_er_test), 'Nr of stucks: ', stucks_er_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_moves = pd.DataFrame(replay_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Experience Replay Moves / Game\")\n",
    "plt.plot(ema_moves)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e7525e",
   "metadata": {},
   "source": [
    "## Task 6a\n",
    "We changed the administration of rewards as follows: Every regular action / move gets a slight punishment of -0.01, while a checkmate results in a reward of 1.0 and stale in a punishment of -1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "improved_qlearn_network = Network(hidden_layer=256, input_dim=improved_input_size, output_dim=improved_n_possible_actions, eta=0.02)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "improved_count, improved_rewards = qlearn(environment=improved_chess_environment, network=improved_qlearn_network, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.4, B=0.0001, y=0.7)\n",
    "print('\\nLess-Information Q-Learning Agent training, Average reward:', np.mean(improved_rewards), 'Number of steps: ', np.mean(improved_count))\n",
    "nr_moves_minimal, reward_minimal, stuck_minimal = test(network=improved_qlearn_network, environment=improved_chess_environment, number_of_episodes=5_000)\n",
    "print('\\nLess-Information Q-Learning Agent testing, Average reward:', np.mean(reward_minimal), 'Number of steps: ', np.mean(nr_moves_minimal), 'Nr of stucks: ', stuck_minimal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ema_moves = pd.DataFrame(improved_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Less-Information Q-Learning Moves / Game\")\n",
    "plt.plot(ema_moves)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 6b\n",
    "We removed the additional input in the state representation (number of possible moves for opponent king, information about checked-state)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec5e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "minimal_network = Network(hidden_layer=256, input_dim=minimal_input_size, output_dim=minimal_n_possible_actions, eta=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be856a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_count, minimal_rewards = qlearn(environment=minimal_chess_environment, network=minimal_network, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.4, B=0.0001, y=0.7)\n",
    "print('\\nLess-Information Q-Learning Agent training, Average reward:', np.mean(minimal_rewards), 'Number of steps: ', np.mean(minimal_count))\n",
    "nr_moves_minimal, reward_minimal, stuck_minimal = test(network=minimal_network, environment=minimal_chess_environment, number_of_episodes=5_000)\n",
    "print('\\nLess-Information Q-Learning Agent testing, Average reward:', np.mean(reward_minimal), 'Number of steps: ', np.mean(nr_moves_minimal), 'Nr of stucks: ', stuck_minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_moves = pd.DataFrame(minimal_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Less-Information Q-Learning Moves / Game\")\n",
    "plt.plot(ema_moves)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea16443f",
   "metadata": {},
   "source": [
    "## Task 7\n",
    "We implemented RMSProp as part of the Network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ebce812",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "network_rmsprop = Network(hidden_layer=256, input_dim=regular_input_size, output_dim=regular_n_possible_actions, eta=0.02, rmsprop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a8733f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.02908785  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.03497055 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.01728538  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]] -0.08698336528929326 0.0891000510643951\n",
      "Under wurzel [[1.00000000e-07 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.00000000e-07 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.90888533e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [3.49715512e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.72863834e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.00000000e-07 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 9.99965623  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-9.99971405 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 9.99942151  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "reduction [[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.19999312  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.19999428 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.19998843  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "min w1, max w1 -0.5079825079956635 0.5134604915959293\n",
      "min w2, max w2 -0.2663178144423062 0.2320985504935516\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.01299634 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.03069847 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.0041538  0.         0.         ... 0.         0.         0.        ]] -0.04192755607484368 0.04716238640375918\n",
      "Under wurzel [[1.00000000e-07 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.00000000e-07 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.89430486e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [3.71441664e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.51889613e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [4.15480386e-04 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [3.49889025 0.         0.         ... 0.         0.         0.        ]\n",
      " [8.72389172 0.         0.         ... 0.         0.         0.        ]\n",
      " [9.99759315 0.         0.         ... 0.         0.         0.        ]]\n",
      "reduction [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.06997781 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.17447783 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.19995186 0.         0.         ... 0.         0.         0.        ]]\n",
      "min w1, max w1 -0.5265828164376547 0.5862206645818215\n",
      "min w2, max w2 -0.35267075363868755 0.2320985504935516\n",
      "[[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.04202272 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.03886587  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.01799897  0.          0.         ...  0.          0.\n",
      "   0.        ]] -0.10525661674540916 0.12200898138514032\n",
      "Under wurzel [[1.00000000e-07 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [4.20237233e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.87979748e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [5.36328393e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.50125794e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.84683899e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-9.99976204 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 7.24665454  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 9.74582556  0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "reduction [[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.19999524 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.14493309  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.19491651  0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "min w1, max w1 -0.5265828164376547 0.6754366994450438\n",
      "min w2, max w2 -0.37072498308179014 0.2320985504935516\n",
      "[[ 0.01583846  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-0.01303634 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]] -0.07494462813112625 0.07759707364105592\n",
      "Under wurzel [[1.58394566e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [4.18130818e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.86536281e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [5.49332901e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.48370816e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.83758210e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[ 9.99936867  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-2.3731212  -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "reduction [[ 0.19998737  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-0.04746242 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "min w1, max w1 -0.6112102444651628 0.7147978331271938\n",
      "min w2, max w2 -0.37072498308179014 0.3401362890226382\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.00705049  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]] -0.03613412681845709 0.034811643601239874\n",
      "Under wurzel [[1.57600653e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [4.21967000e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.85100050e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [5.46579385e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.46624636e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.82837160e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 1.67086302  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "reduction [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.03341726  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "min w1, max w1 -0.6112102444651628 0.8462073002320849\n",
      "min w2, max w2 -0.37072498308179014 0.3401362890226382\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.08174881  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.01204774  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]] -0.17363570159038172 0.20909664208250825\n",
      "Under wurzel [[1.56810720e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [9.19006213e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.83671019e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [5.57024828e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.44887208e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.81920727e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 8.89534909  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 2.16287331  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "reduction [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17790698  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.04325747  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "min w1, max w1 -0.663066292356035 0.8309619646935263\n",
      "min w2, max w2 -0.37072498308179014 0.3700888920521246\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.0341994   0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-0.01506869 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]] -0.0360647995013791 0.034199404728517374\n",
      "Under wurzel [[1.56024747e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [9.76262236e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.82249150e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [5.74352598e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.43158490e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.81008888e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.50309614  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-2.62359656 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "reduction [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.07006192  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-0.05247193 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "min w1, max w1 -0.663066292356035 0.9180200912210956\n",
      "min w2, max w2 -0.37072498308179014 0.3700888920521246\n",
      "[[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " ...\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]] -0.14347747899646632 0.1673120490763523\n",
      "Under wurzel [[1.55242713e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [9.71368710e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.80834408e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [5.71473670e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.41438436e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.80101620e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " ...\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]]\n",
      "reduction [[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " ...\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]]\n",
      "min w1, max w1 -0.6784921064344468 0.894674693387618\n",
      "min w2, max w2 -0.37072498308179014 0.3700888920521246\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] 0.0 0.0\n",
      "Under wurzel [[1.54464599e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [9.66499713e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.79426758e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [5.68609172e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.39727005e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.79198900e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "reduction [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "min w1, max w1 -0.6784921064344468 0.8739047071150436\n",
      "min w2, max w2 -0.37072498308179014 0.3700888920521246\n",
      "[[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]] -1.2767359544240673 1.3059264300926414\n",
      "Under wurzel [[1.53690386e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [9.61655123e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.78026164e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [5.65759033e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.38024152e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.78300704e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "reduction [[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "min w1, max w1 -0.6784921064344468 0.8739047071150436\n",
      "min w2, max w2 -0.37072498308179014 0.3700888920521246\n",
      "[[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.25521212 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]] -0.3391598242800542 0.10598335006517626\n",
      "Under wurzel [[1.52920053e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.72559918e-02 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.76632591e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [5.62923181e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.36329835e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.77407011e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-9.36352353 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "reduction [[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.18727047 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "min w1, max w1 -0.6784921064344468 0.9221683991782973\n",
      "min w2, max w2 -0.37072498308179014 0.3700888920521246\n",
      "[[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-9.1135767  -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 3.76263396  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]] -12.2076371424219 6.5452875544457\n",
      "Under wurzel [[1.52153582e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.71193699e-02 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.75246003e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [9.11374980e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.76278376e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.76517797e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-9.99981006 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 9.99960188  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "reduction [[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-0.1999962  -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.19999204  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n",
      "min w1, max w1 -0.6784921064344468 1.0271346180581324\n",
      "min w2, max w2 -0.37072498308179014 0.5145468121489915\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17975872  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]] -0.044850267237105834 0.2576552343630712\n",
      "Under wurzel [[1.51390953e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.24228114e-02 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.73866365e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [9.06806656e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.74392258e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.75633040e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 5.54420511  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "reduction [[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.1108841  0.         0.        ...  0.         0.         0.       ]\n",
      " [-0.        -0.        -0.        ... -0.        -0.        -0.       ]\n",
      " ...\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]]\n",
      "min w1, max w1 -0.6784921064344468 1.226927938038344\n",
      "min w2, max w2 -0.37072498308179014 0.5541903596066595\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 4.25587127  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [36.75212897  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]] -3.3308558528360654 43.85621532945786\n",
      "Under wurzel [[1.50632146e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [4.26808163e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.72493643e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [3.78434482e+00 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.72515593e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.74752719e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [9.97139146 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [9.71162267 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "reduction [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.19942783 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.19423245 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "min w1, max w1 -0.6784921064344468 1.2199627243174294\n",
      "min w2, max w2 -0.37072498308179014 0.5541903596066595\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 4.13522872  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [ 3.70684831  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]] -7.240119336844875 9.596500409450483\n",
      "Under wurzel [[1.49877143e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [5.92743415e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.71127801e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [3.78357772e+00 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.70648336e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.73876810e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 6.97642288  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [ 0.97972041  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "reduction [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.13952846  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [ 0.01959441  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "min w1, max w1 -0.7753820992964873 1.0247190509361404\n",
      "min w2, max w2 -0.37072498308179014 0.38380930371901206\n",
      "[[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [-0.        -0.        -0.        ... -0.        -0.        -0.       ]\n",
      " [-0.        -0.        -0.        ... -0.        -0.        -0.       ]\n",
      " ...\n",
      " [-2.5508058 -0.        -0.        ... -0.        -0.        -0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]] -24.163830362440862 9.761189975267406\n",
      "Under wurzel [[1.49125925e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [5.89772252e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.69768806e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [3.77324421e+00 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.68790438e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.73005292e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-0.67602457 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "reduction [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [-0.01352049 -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "min w1, max w1 -0.8038175831338665 0.9691895653337003\n",
      "min w2, max w2 -0.48956384917980267 0.372916094768901\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] 0.0 0.0\n",
      "Under wurzel [[1.48378472e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [5.86815982e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.68416623e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [3.75433058e+00 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.66941854e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.72138142e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "reduction [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "min w1, max w1 -0.8038175831338665 0.9181538936768225\n",
      "min w2, max w2 -0.6138979107921928 0.372916094768901\n",
      "[[0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " ...\n",
      " [2.6653248 0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]] -4.566366541183934 10.879175898204561\n",
      "Under wurzel [[1.47634765e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [5.83874530e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.67071218e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [3.74500837e+00 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.65102535e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.71275339e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.71170063 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "reduction [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.01423401 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "min w1, max w1 -0.8038175831338665 0.9181538936768225\n",
      "min w2, max w2 -0.6138979107921928 0.372916094768901\n",
      "[[-0. -0. -0. ... -0. -0. -0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]] -2.7587912799360383 4.259949981193846\n",
      "Under wurzel [[1.46894787e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [5.80947823e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.65732557e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [3.72623628e+00 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.63272436e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.70416861e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[-0. -0. -0. ... -0. -0. -0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "reduction [[-0. -0. -0. ... -0. -0. -0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "min w1, max w1 -0.8038175831338665 0.893804525649917\n",
      "min w2, max w2 -0.6138979107921928 0.372916094768901\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 3.74697769  3.74697769  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]] -3.3632966914130527 4.962345948520696\n",
      "Under wurzel [[1.46158518e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [5.78035786e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.64400606e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [3.72644427e+00 3.74697869e-01 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.61451511e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.69562685e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 1.00551019  9.99999733  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "reduction [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.0201102   0.19999995  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "min w1, max w1 -0.7753820992964873 0.8971853870622104\n",
      "min w2, max w2 -0.6138979107921928 0.372916094768901\n",
      "[[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " ...\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]] -2.038959439132022 7.169536115843542\n",
      "Under wurzel [[1.45425939e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [5.75138346e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [2.63075331e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " ...\n",
      " [3.70776523e+00 3.72819673e-01 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [3.59639713e-01 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]\n",
      " [1.68712792e-03 1.00000000e-07 1.00000000e-07 ... 1.00000000e-07\n",
      "  1.00000000e-07 1.00000000e-07]]\n",
      "Multi [[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " ...\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]]\n",
      "reduction [[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " ...\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]]\n",
      "min w1, max w1 -0.8784408058509948 0.8885433407425278\n",
      "min w2, max w2 -0.6138979107921928 0.372916094768901\n",
      "\n",
      "RMSProp Q-Learning Agent training, Average reward: 0.0 Number of steps:  21.0\n"
     ]
    }
   ],
   "source": [
    "rmsprop_count, rmsprop_rewards = sarsa(environment=regular_chess_environment, network=network_rmsprop, epsilon=0.4, number_of_episodes=1, B=0.0001, y=0.7)\n",
    "print('\\nRMSProp Q-Learning Agent training, Average reward:', np.mean(rmsprop_rewards), 'Number of steps: ', np.mean(rmsprop_count))\n",
    "# nr_moves_q3_test, rewards_q3_test, stucks_q3_test = test(network=network_rmsprop, environment=regular_chess_environment, number_of_episodes=5_000)\n",
    "# print('\\nRMSProp Q-Learning Agent testing, Average reward:', np.mean(rewards_q3_test), 'Number of steps: ', np.mean(nr_moves_q3_test), 'Nr of stucks: ', stucks_q3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_moves = pd.DataFrame(rmsprop_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"RMSProp Q-Learning Moves / Game\")\n",
    "plt.plot(ema_moves)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-6c1f981b",
   "language": "python",
   "display_name": "PyCharm (ItRL)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}