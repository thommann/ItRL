{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02944396",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joel\\anaconda3\\envs\\itrl\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas matplotlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from Chess_env import *\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "BOARD_SIZE = 4\n",
    "NUMBER_OF_EPISODES = 100_000\n",
    "NUMBER_OF_TEST_EPISODES = 500\n",
    "\n",
    "SARSA_COLOR=\"#008000\"\n",
    "ADAPTED_SARSA_COLOR=\"#00d435\"\n",
    "QLEARNING_COLOR=\"#0040d4\"\n",
    "REDUCED_QLEARNING_COLOR=\"#0078d4\"\n",
    "REPLAY_COLOR=\"#d40012\"\n",
    "RMSPROP_REPLAY_COLOR= \"#c96787\"\n",
    "IMPROVED_REPLAY_COLOR=\"#ff0d92\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [15, 10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "0bceca7c",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "\n",
    "You can find the environment in the file Chess_env, which contains the class Chess_env. To define an object, you need to provide the board size considered as input. In our example, size_board=4. \n",
    "Chess_env is composed by the following methods:\n",
    "\n",
    "1. Initialise_game. The method initialises an episode by placing the three pieces considered (Agent's king and queen, enemy's king) in the chess board. The outputs of the method are described below in order.\n",
    "\n",
    "     S $\\;$ A matrix representing the board locations filled with 4 numbers: 0, no piece in that position; 1, location of the \n",
    "     agent's king; 2 location of the queen; 3 location of the enemy king.\n",
    "     \n",
    "     X $\\;$ The features, that is the input to the neural network. See the assignment for more information regarding the            definition of the features adopted. To personalise this, go into the Features method of the class Chess_env() and change        accordingly.\n",
    "     \n",
    "     allowed_a $\\;$ The allowed actions that the agent can make. The agent is moving a king, with a total number of 8                possible actions, and a queen, with a total number of $(board_{size}-1)\\times 8$ actions. The total number of possible actions correspond      to the sum of the two, but not all actions are allowed in a given position (movements to locations outside the borders or      against chess rules). Thus, the variable allowed_a is a vector that is one (zero) for an action that the agent can (can't)      make. Be careful, apply the policy considered on the actions that are allowed only.\n",
    "     \n",
    "\n",
    "2. OneStep. The method performs a one step update of the system. Given as input the action selected by the agent, it updates the chess board by performing that action and the response of the enemy king (which is a random allowed action in the settings considered). The first three outputs are the same as for the Initialise_game method, but the variables are computed for the position reached after the update of the system. The fourth and fifth outputs are:\n",
    "\n",
    "     R $\\;$ The reward. To change this, look at the OneStep method of the class where the rewards are set.\n",
    "     \n",
    "     Done $\\;$ A variable that is 1 if the episode has ended (checkmate or draw).\n",
    "     \n",
    "     \n",
    "3. Features. Given the chessboard position, the method computes the features.\n",
    "\n",
    "This information and a quick analysis of the class should be all you need to get going. The other functions that the class exploits are uncommented and constitute an example on how not to write a python code. You can take a look at them if you want, but it is not necessary.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9593a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITIALISE THE ENVIRONMENT\n",
    "regular_chess_environment = Chess_Env(BOARD_SIZE)\n",
    "regular_board, regular_X, regular_allowed_a = regular_chess_environment.Initialise_game()\n",
    "regular_n_possible_actions = np.shape(regular_allowed_a)[0]  # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
    "regular_input_size = np.shape(regular_X)[0]  ## INPUT SIZE\n",
    "\n",
    "## INITIALIZE ENVIRONMENT WITH IMPROVED REWARDS\n",
    "improved_chess_environment = Chess_Env(BOARD_SIZE, R=0.0, R_draw=0.5, R_checked=1.0)\n",
    "improved_board, improved_X, improved_allowed_a = improved_chess_environment.Initialise_game()\n",
    "improved_n_possible_actions = np.shape(improved_allowed_a)[0]  # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
    "improved_input_size = np.shape(improved_X)[0]  ## INPUT SIZE\n",
    "\n",
    "## INITIALIZE ENVIRONMENT WITH MINIMAL INFORMATION\n",
    "minimal_chess_environment = Chess_Env(BOARD_SIZE, extended_features=False)\n",
    "minimal_board, minimal_X, minimal_allowed_a = minimal_chess_environment.Initialise_game()\n",
    "minimal_n_possible_actions = np.shape(minimal_allowed_a)[0]  # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
    "minimal_input_size = np.shape(minimal_X)[0]  ## INPUT SIZE\n",
    "\n",
    "## INITIALISE TESTING ENVIRONMENT\n",
    "testing_chess_environment = Chess_Env(BOARD_SIZE)\n",
    "testing_board, testing_X, testing_allowed_a = testing_chess_environment.Initialise_game()\n",
    "testing_n_possible_actions = np.shape(testing_allowed_a)[0]  # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
    "testing_input_size = np.shape(testing_X)[0]  ## INPUT SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbc05bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRINT 5 STEPS OF AN EPISODE CONSIDERING A RANDOM AGENT\n",
    "\n",
    "def random_test(environment):\n",
    "    S, X, allowed_a = environment.Initialise_game()  # INTIALISE GAME\n",
    "\n",
    "    print(S)  # PRINT CHESS BOARD (SEE THE DESCRIPTION ABOVE)\n",
    "\n",
    "    print('check? ', environment.check)  # PRINT VARIABLE THAT TELLS IF ENEMY KING IS IN CHECK (1) OR NOT (0)\n",
    "    print('dofk2 ', np.sum(environment.dfk2_constrain).astype(int))  # PRINT THE NUMBER OF LOCATIONS THAT THE ENEMY KING CAN MOVE TO\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        a, _ = np.where(allowed_a == 1)  # FIND WHAT THE ALLOWED ACTIONS ARE\n",
    "        a_agent = np.random.permutation(a)[0]  # MAKE A RANDOM ACTION\n",
    "\n",
    "        S, X, allowed_a, R, Done = environment.OneStep(a_agent)  # UPDATE THE ENVIRONMENT\n",
    "\n",
    "        ## PRINT CHESS BOARD AND VARIABLES\n",
    "        print('')\n",
    "        print(S)\n",
    "        print(R, '', Done)\n",
    "        print('check? ', environment.check)\n",
    "        print('dofk2 ', np.sum(environment.dfk2_constrain).astype(int))\n",
    "\n",
    "        # TERMINATE THE EPISODE IF Done=True (DRAW OR CHECKMATE)\n",
    "        if Done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 3 0 0]\n",
      " [0 0 0 2]]\n",
      "check?  0\n",
      "dofk2  2\n",
      "\n",
      "[[2 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 3 0 0]]\n",
      "0  0\n",
      "check?  0\n",
      "dofk2  2\n",
      "\n",
      "[[2 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 3 0 1]\n",
      " [0 0 0 0]]\n",
      "0  0\n",
      "check?  0\n",
      "dofk2  1\n",
      "\n",
      "[[0 0 0 0]\n",
      " [3 0 0 0]\n",
      " [0 0 2 1]\n",
      " [0 0 0 0]]\n",
      "0  0\n",
      "check?  0\n",
      "dofk2  1\n",
      "\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [3 0 0 1]\n",
      " [0 0 0 2]]\n",
      "0  0\n",
      "check?  0\n",
      "dofk2  2\n",
      "\n",
      "[[0 0 0 0]\n",
      " [0 3 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 2 0]]\n",
      "0  0\n",
      "check?  0\n",
      "dofk2  3\n"
     ]
    }
   ],
   "source": [
    "random_test(environment=regular_chess_environment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc16cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game(environment):\n",
    "    # PERFORM N_episodes=1000 EPISODES MAKING RANDOM ACTIONS AND COMPUTE THE AVERAGE REWARD AND NUMBER OF MOVES\n",
    "\n",
    "    S, X, allowed_a = environment.Initialise_game()\n",
    "    N_episodes = 1_000\n",
    "\n",
    "    # VARIABLES WHERE TO SAVE THE FINAL REWARD IN AN EPISODE AND THE NUMBER OF MOVES\n",
    "    count = []\n",
    "    rewards = []\n",
    "\n",
    "    for n in range(N_episodes):\n",
    "\n",
    "        S, X, allowed_a = environment.Initialise_game()  # INITIALISE GAME\n",
    "        Done = 0  # SET Done=0 AT THE BEGINNING\n",
    "        i = 1  # COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE\n",
    "\n",
    "        # UNTIL THE EPISODE IS NOT OVER...(Done=0)\n",
    "        while Done == 0:\n",
    "\n",
    "            # SAME AS THE CELL BEFORE, BUT SAVING THE RESULTS WHEN THE EPISODE TERMINATES\n",
    "\n",
    "            a, _ = np.where(allowed_a == 1)\n",
    "            a_agent = np.random.permutation(a)[0]\n",
    "\n",
    "            S, X, allowed_a, R, Done = environment.OneStep(a_agent)\n",
    "\n",
    "            if Done:\n",
    "                count.append(R)\n",
    "                rewards.append(i)\n",
    "                break\n",
    "\n",
    "            i = i + 1  # UPDATE THE COUNTER\n",
    "    return count, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# AS YOU SEE, THE PERFORMANCE OF A RANDOM AGENT ARE NOT GREAT, SINCE THE MAJORITY OF THE POSITIONS END WITH A DRAW\n",
    "# (THE ENEMY KING IS NOT IN CHECK AND CAN'T MOVE)\n",
    "random_count, random_rewards = random_game(environment=regular_chess_environment)\n",
    "f'Random_Agent - Average reward: {np.mean(random_count)}. Number of steps: {np.mean(random_rewards)}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "3829534d",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "The following is an implementation of a general **Neural Network Class** and a reinforcement learner using **SARSA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306cbb8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    W1, W2 = None, None\n",
    "\n",
    "    def __init__(self, hidden_layer, input_dim, output_dim, eta=0.02, rho=0.9, rmsprop=False):\n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(hidden_layer + 1, input_dim + 1) * 1.0 / np.sqrt(input_dim + 1)\n",
    "        self.W2 = np.random.randn(output_dim, hidden_layer + 1) * 1.0 / np.sqrt(hidden_layer + 1)\n",
    "        # Step size\n",
    "        self.eta = eta\n",
    "        # RMSprop parameters\n",
    "        self.rho = rho\n",
    "        self.V1 = np.ones(self.W1.shape)\n",
    "        self.V2 = np.ones(self.W2.shape)\n",
    "        self.l1 = None\n",
    "        self.l2 = None\n",
    "        self.rmsprop = rmsprop\n",
    "       \n",
    "    @staticmethod\n",
    "    def relu(A):\n",
    "        return np.maximum(0, A)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(X, T, Y, H, W2, Z1, Z2):\n",
    "        # Add bias term\n",
    "        X_bias = np.vstack((np.ones(X.shape[1]), X))\n",
    "\n",
    "        # d Loss / d Y\n",
    "        G_Y = 2. * (Y - T)\n",
    "        # d Y / d Z2\n",
    "        G_Z2 = G_Y * np.maximum(0, np.sign(Z2))\n",
    "        # d Z2 / d W2\n",
    "        G_W2 = np.dot(G_Z2, H.T)\n",
    "        # Layer 2 gradient\n",
    "        G2 = (1. / X_bias.shape[1]) * G_W2\n",
    "\n",
    "        # d Z2 / d H\n",
    "        G_H = np.dot(W2.T, G_Z2)\n",
    "        # d H / d Z1\n",
    "        G_Z1 = G_H * np.maximum(0, np.sign(Z1))\n",
    "        # d Z1 / d W1\n",
    "        G_W1 = np.dot(G_Z1, X_bias.T)\n",
    "        # Layer 1 gradient\n",
    "        G1 = (1. / X_bias.shape[1]) * G_W1\n",
    "        return G1, G2\n",
    "\n",
    "    def descent(self, X, T, H, Y, Z1, Z2):\n",
    "        G1, G2 = Network.gradient(X, T, Y, H, self.W2, Z1, Z2)\n",
    "\n",
    "        if self.rmsprop:\n",
    "            self.V1 = self.rho * self.V1 + (1 - self.rho) * np.square(G1)\n",
    "            self.V2 = self.rho * self.V2 + (1 - self.rho) * np.square(G2)\n",
    "\n",
    "            self.l1 = (self.eta / (np.sqrt(self.V1) + 1e-7))\n",
    "            self.l2 = (self.eta / (np.sqrt(self.V2) + 1e-7))\n",
    "\n",
    "            self.W1 -= self.l1 * G1\n",
    "            self.W2 -= self.l2 * G2\n",
    "        else:\n",
    "            self.W1 -= self.eta * G1\n",
    "            self.W2 -= self.eta * G2\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Add Bias to first layer input\n",
    "        X_bias = np.vstack((np.ones(X.shape[1]), X))\n",
    "        # First Layer\n",
    "        Z1 = np.dot(self.W1, X_bias)\n",
    "        H = Network.relu(Z1)\n",
    "        # Fix Bias for second layer input\n",
    "        H[0, :] = 1.\n",
    "        # Second Layer\n",
    "        Z2 = np.dot(self.W2, H)\n",
    "        Y = Network.relu(Z2)\n",
    "        return Y, H, Z1, Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9bb9e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qvalues, epsilon):\n",
    "    N_class = np.shape(Qvalues)[0]\n",
    "    batch_size = np.shape(Qvalues)[1]\n",
    "\n",
    "    rand_values = np.random.uniform(0, 1, [batch_size])\n",
    "\n",
    "    rand_a = rand_values < epsilon\n",
    "    a = np.zeros([batch_size, N_class])\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        if rand_a[i]:\n",
    "            valid_moves = np.where(Qvalues[:,i] > -1000)[0]\n",
    "            chosen = np.random.choice(valid_moves)\n",
    "            a[i, chosen] = 1\n",
    "        else:\n",
    "            a[i, np.argmax(Qvalues[:, i])] = 1\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(network, environment, number_of_episodes):\n",
    "    nr_moves = []\n",
    "    total_rewards = []\n",
    "    stuck_runs = 0\n",
    "    for i in range(number_of_episodes):\n",
    "\t    done = 0\n",
    "\t    S, X, allowed_a = environment.Initialise_game()\n",
    "\t    X = X.reshape(len(X), 1)\n",
    "\t    total_reward = 0\n",
    "\t    count = 0\n",
    "\t    while done == 0:\n",
    "\t\t    count += 1\n",
    "\t\t    Q_values, _, _, _ = network.forward(X)\n",
    "\t\t    masked_Q_values = Q_values - (1 - allowed_a) * 100000\n",
    "\t\t    a_agent = epsilon_greedy_policy(masked_Q_values, 0).T\n",
    "\t\t    S, X, allowed_a, R, done = environment.OneStep(np.argmax(a_agent))\n",
    "\t\t    X = np.array(X).reshape(len(X),1)\n",
    "\t\t    total_reward += R\n",
    "\t\t    if count > 100:\n",
    "\t\t\t    stuck_runs += 1\n",
    "\t\t\t    break\n",
    "\t    nr_moves.append(count)\n",
    "\t    total_rewards.append(total_reward)\n",
    "    return nr_moves, total_rewards, stuck_runs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba1f84",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def sarsa(environment, network, number_of_episodes, epsilon, B, y):\n",
    "    count = []\n",
    "    rewards = []\n",
    "\n",
    "    for n in range(number_of_episodes):\n",
    "\n",
    "        epsilon_f = epsilon / (1 + B * n)  ## DECAYING EPSILON\n",
    "        Done = 0  ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
    "        i = 1  ## COUNTER FOR NUMBER OF ACTIONS\n",
    "        total_reward = 0 ## COUNTER FOR TOTAL REWARD\n",
    "\n",
    "        S, X, allowed_a = environment.Initialise_game()  ## INITIALISE GAME\n",
    "        X = X.reshape(len(X), 1)\n",
    "\n",
    "        if n > 0 and n % 100 == 0:\n",
    "            print(f\"\\rEp.: {n}, epsilon: {epsilon_f:.3f}, moves: {np.mean(count[n - 100:]):.2f}\", end=\"\")\n",
    "\n",
    "        Q_values, H, Z1, Z2 = network.forward(X)\n",
    "        masked_Q_values = Q_values - (1 - allowed_a) * 100000\n",
    "        a_agent = epsilon_greedy_policy(masked_Q_values, epsilon_f).T\n",
    "\n",
    "        while Done == 0:  ## START THE EPISODE\n",
    "\n",
    "            S_next, X_next, allowed_a_next, R, Done = environment.OneStep(np.argmax(a_agent))\n",
    "            X_next = np.array(X_next).reshape(len(X_next), 1)\n",
    "            \n",
    "            total_reward += R\n",
    "\n",
    "            ## THE EPISODE HAS ENDED, UPDATE...BE CAREFUL, THIS IS THE LAST STEP OF THE EPISODE\n",
    "            if Done == 1:\n",
    "                output = Q_values * a_agent\n",
    "                target = R * a_agent\n",
    "                network.descent(X, target, H, output, Z1, Z2)\n",
    "                count.append(i)\n",
    "                rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "            # IF THE EPISODE IS NOT OVER...\n",
    "            else:\n",
    "                Q_values_next, H_next, Z1_next, Z2_next = network.forward(X_next)\n",
    "                masked_Q_values_next = Q_values_next - (1 - allowed_a_next) * 100000\n",
    "                a_agent_next = epsilon_greedy_policy(masked_Q_values_next, epsilon_f).T\n",
    "                future_R = Q_values_next[np.argmax(a_agent_next)]\n",
    "                output = Q_values * a_agent\n",
    "                target = (R + y * future_R) * a_agent\n",
    "                network.descent(X, target, H, output, Z1, Z2)\n",
    "\n",
    "            # NEXT STATE AND CO. BECOME ACTUAL STATE...     \n",
    "            S = np.copy(S_next)\n",
    "            X = np.copy(X_next)\n",
    "            allowed_a = np.copy(allowed_a_next)\n",
    "            Q_values = np.copy(Q_values_next)\n",
    "            H = np.copy(H_next)\n",
    "            a_agent = np.copy(a_agent_next)\n",
    "            Z1 = np.copy(Z1_next)\n",
    "            Z2 = np.copy(Z2_next)\n",
    "\n",
    "            i += 1  # UPDATE COUNTER FOR NUMBER OF ACTIONS\n",
    "    return count, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "network_sarsa = Network(hidden_layer=200, input_dim=regular_input_size, output_dim=regular_n_possible_actions, eta=0.0035)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ece0d0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sarsa_count, sarsa_rewards = sarsa(environment=regular_chess_environment, network=network_sarsa, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.2, B=0.00005, y=0.85)\n",
    "f'SARSA Agent training - Average reward: {np.mean(sarsa_rewards)}. Number of steps: {np.mean(sarsa_count)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nr_moves_sarsa_test, rewards_sarsa_test, stucks_sarsa_test = test(network=network_sarsa, environment=testing_chess_environment, number_of_episodes=NUMBER_OF_TEST_EPISODES)\n",
    "f'SARSA Agent testing, Average reward: {np.mean(rewards_sarsa_test)}. Number of steps: {np.mean(nr_moves_sarsa_test)}. Nr of stucks: {stucks_sarsa_test}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "7e6fbfa2",
   "metadata": {},
   "source": [
    "### Plot 1: Reward per Game over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5446f6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sarsa_ema_reward = pd.DataFrame(sarsa_rewards).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"SARSA Reward / Game\")\n",
    "plt.plot(sarsa_ema_reward, label=\"Sarsa\", color=SARSA_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a48f9f",
   "metadata": {},
   "source": [
    "### Plot 2: Number of Moves per Game vs. training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288ad57",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sarsa_ema_moves = pd.DataFrame(sarsa_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"SARSA Moves / Game\")\n",
    "plt.plot(sarsa_ema_moves, label=\"Sarsa\", color=SARSA_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f2bda2",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "We changed increased **epsilon_0 to 0.4**, decreased **gamma to 0.7**, decreased **beta to 0.0001**, increased **eta to 0.02** and increased the number of **hidden layers to 256**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37280e58",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "network_adapted_sarsa = Network(hidden_layer=256, input_dim=regular_input_size, output_dim=regular_n_possible_actions, eta=0.02 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f8edf1",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "adapted_sarsa_count, adapted_sarsa_rewards = sarsa(environment=regular_chess_environment, network=network_adapted_sarsa, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.4, B=0.0001, y=0.7)\n",
    "f'Adapted SARSA Agent training, Average reward: {np.mean(adapted_sarsa_rewards)}. Number of steps: {np.mean(adapted_sarsa_count)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nr_moves_adapted_sarsa_test, rewards_adapted_sarsa_test, stucks_adapted_sarsa_test = test(network=network_adapted_sarsa, environment=testing_chess_environment, number_of_episodes=NUMBER_OF_TEST_EPISODES)\n",
    "f'Adapted SARSA Agent testing, Average reward: {np.mean(rewards_adapted_sarsa_test)}. Number of steps: {np.mean(nr_moves_adapted_sarsa_test)}. Nr of stucks: {stucks_adapted_sarsa_test}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adapted_sarsa_ema_reward = pd.DataFrame(adapted_sarsa_rewards).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Adapted SARSA Reward / Game\")\n",
    "plt.plot(adapted_sarsa_ema_reward, label=\"Adapted Sarsa\", color=ADAPTED_SARSA_COLOR)\n",
    "plt.plot(sarsa_ema_reward, label=\"Sarsa\", color=SARSA_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adapted_sarsa_ema_moves = pd.DataFrame(adapted_sarsa_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Adapted SARSA Moves / Game\")\n",
    "plt.plot(adapted_sarsa_ema_moves, label=\"Adapted Sarsa\", color=ADAPTED_SARSA_COLOR)\n",
    "plt.plot(sarsa_ema_moves, label=\"Sarsa\", color=SARSA_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "8a983c3c",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "We implemented **Q-Learning** and **Experience Replay** as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ed177",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111f01e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def qlearn(environment, network, number_of_episodes, epsilon, B, y):\n",
    "    count = []\n",
    "    rewards = []\n",
    "    \n",
    "    for n in range(number_of_episodes):\n",
    "\n",
    "        epsilon_f = epsilon / (1 + B * n)  ## DECAYING EPSILON\n",
    "        Done = 0  ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
    "        i = 1  ## COUNTER FOR NUMBER OF ACTIONS\n",
    "        total_reward = 0 ## COUNTER FOR TOTAL REWARD\n",
    "\n",
    "        S, X, allowed_a = environment.Initialise_game()  ## INITIALISE GAME\n",
    "        X = X.reshape(len(X), 1)\n",
    "\n",
    "        if n > 0 and n % 100 == 0:\n",
    "            print(f\"\\rEp.: {n}, epsilon: {epsilon_f:.3f}, moves: {np.mean(count[n - 100:]):.2f}\", end=\"\")\n",
    "\n",
    "        Q_values, H, Z1, Z2 = network.forward(X)\n",
    "        masked_Q_values = Q_values - (1 - allowed_a) * 100_000\n",
    "        a_agent = epsilon_greedy_policy(masked_Q_values, epsilon_f).T\n",
    "\n",
    "        while Done == 0:  ## START THE EPISODE\n",
    "\n",
    "            S_next, X_next, allowed_a_next, R, Done = environment.OneStep(np.argmax(a_agent))\n",
    "            X_next = np.array(X_next).reshape(len(X_next), 1)\n",
    "            total_reward += R\n",
    "\n",
    "            ## THE EPISODE HAS ENDED, UPDATE...BE CAREFUL, THIS IS THE LAST STEP OF THE EPISODE\n",
    "            if Done == 1:\n",
    "                output = Q_values * a_agent\n",
    "                target = R * a_agent\n",
    "                network.descent(X, target, H, output, Z1, Z2)\n",
    "                count.append(i)\n",
    "                rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "            # IF THE EPISODE IS NOT OVER...\n",
    "            else:\n",
    "                Q_values_next, H_next, Z1_next, Z2_next = network.forward(X_next)\n",
    "                masked_Q_values_next = Q_values_next - (1 - allowed_a_next) * 100_000\n",
    "                # Q-Learning chooses the next step greedily (epsilon=0)\n",
    "                a_agent_next = epsilon_greedy_policy(masked_Q_values_next, 0.0).T\n",
    "                future_R = Q_values_next[np.argmax(a_agent_next)]\n",
    "                output = Q_values * a_agent\n",
    "                target = (R + y * future_R) * a_agent\n",
    "                network.descent(X, target, H, output, Z1, Z2)\n",
    "\n",
    "            # NEXT STATE AND CO. BECOME ACTUAL STATE...     \n",
    "            S = np.copy(S_next)\n",
    "            X = np.copy(X_next)\n",
    "            allowed_a = np.copy(allowed_a_next)\n",
    "            Q_values = np.copy(Q_values_next)\n",
    "            H = np.copy(H_next)\n",
    "            Z1 = np.copy(Z1_next)\n",
    "            Z2 = np.copy(Z2_next)\n",
    "            # Q-Learning chooses next action based on greedy policty\n",
    "            a_agent = epsilon_greedy_policy(masked_Q_values_next, epsilon_f).T\n",
    "\n",
    "            i += 1  # UPDATE COUNTER FOR NUMBER OF ACTIONS\n",
    "    return count, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "network_qlearn = Network(hidden_layer=256, input_dim=regular_input_size, output_dim=regular_n_possible_actions, eta=0.02)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5710d",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "qlearn_count, qlearn_rewards = qlearn(environment=regular_chess_environment, network=network_qlearn, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.4, B=0.0001, y=0.7)\n",
    "f'Adapted Q-Learning Agent, Average reward: {np.mean(qlearn_rewards)}. Number of steps: {np.mean(qlearn_count)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nr_moves_q_test, rewards_q_test, stucks_q_test = test(network=network_qlearn, environment=testing_chess_environment, number_of_episodes=NUMBER_OF_TEST_EPISODES)\n",
    "f'Adapted Q-Learning Agent testing, Average reward: {np.mean(rewards_q_test)}. Number of steps: {np.mean(nr_moves_q_test)}. Nr of stucks: {stucks_q_test}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qlearn_ema_reward = pd.DataFrame(qlearn_rewards).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Adapted Q-Learning Reward / Game\")\n",
    "plt.plot(qlearn_ema_reward, label=\"Q-Learning\", color=QLEARNING_COLOR)\n",
    "plt.plot(sarsa_ema_reward, label=\"Sarsa\", color=SARSA_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4f482",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "qlearn_ema_moves = pd.DataFrame(qlearn_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Adapted Q-Learning Moves / Game\")\n",
    "plt.plot(qlearn_ema_moves, label=\"Q-Learnng\", color=QLEARNING_COLOR)\n",
    "plt.plot(sarsa_ema_moves, label=\"Sarsa\", color=SARSA_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a35fb",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ee3a7",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Experience:\n",
    "    def __init__(self, state_before, state_after, action, reward):\n",
    "        self.state_before = state_before\n",
    "        self.state_after = state_after\n",
    "        self.action = action\n",
    "        self.reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4c0ad",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def replay(environment, network, number_of_episodes, epsilon, B, y, batch_size=50, relevant_histories=300):\n",
    "    count = []\n",
    "    rewards = []\n",
    "    \n",
    "    experiences = []\n",
    "    \n",
    "    for n in range(number_of_episodes):\n",
    "\n",
    "        epsilon_f = epsilon / (1 + B * n)  ## DECAYING EPSILON\n",
    "\n",
    "        Done = 0  ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
    "        i = 1  ## COUNTER FOR NUMBER OF ACTIONS\n",
    "        total_reward = 0 ## COUNTER FOR TOTAL REWARD\n",
    "\n",
    "        S, X, allowed_a = environment.Initialise_game()  ## INITIALISE GAME\n",
    "        X = X.reshape(len(X), 1)\n",
    "\n",
    "        if n > 0 and n % 100 == 0:\n",
    "            print(f\"\\rEp.: {n}, epsilon: {epsilon_f:.3f}, moves: {np.mean(count[n - 100:]):.2f}\", end=\"\")\n",
    "\n",
    "        while Done == 0:  ## START THE EPISODE\n",
    "            \n",
    "            Q_values, H, Z1, Z2 = network.forward(X)\n",
    "            masked_Q_values = Q_values - (1 - allowed_a) * 100_000\n",
    "            a_agent = epsilon_greedy_policy(masked_Q_values, epsilon_f).T\n",
    "\n",
    "            S_next, X_next, allowed_a_next, R, Done = environment.OneStep(np.argmax(a_agent))\n",
    "            X_next = np.array(X_next).reshape(len(X_next), 1)\n",
    "            total_reward += R\n",
    "            \n",
    "            if X_next.shape[0] == 0:\n",
    "                expi = Experience(X, X, a_agent, R)\n",
    "            else:\n",
    "                expi = Experience(X, X_next, a_agent, R)\n",
    "            experiences.append(expi)\n",
    "            \n",
    "            if len(experiences) > relevant_histories:\n",
    "                experiences = experiences[1:]\n",
    "                training_set = np.random.choice(experiences, batch_size, False)\n",
    "                \n",
    "                state_before = np.hstack([experience.state_before for experience in training_set])\n",
    "                Q_values_before, H_before, Z1_before, Z2_before = network.forward(state_before)\n",
    "                \n",
    "                actions = np.hstack([experience.action for experience in training_set])\n",
    "                Q_values_played = Q_values_before * actions\n",
    "                \n",
    "                states_after = np.hstack([experience.state_after for experience in training_set])\n",
    "                Q_values_after, _, _, _ = network.forward(states_after)\n",
    "\n",
    "                Q_values_after_played = np.max(Q_values_after, 0)\n",
    "                \n",
    "                rewards_played = np.hstack([experience.reward for experience in training_set])\n",
    "                \n",
    "                target = (rewards_played + y * Q_values_after_played) * actions\n",
    "                network.descent(state_before, target, H_before, Q_values_played, Z1_before, Z2_before)\n",
    "                \n",
    "            ## THE EPISODE HAS ENDED, UPDATE...BE CAREFUL, THIS IS THE LAST STEP OF THE EPISODE\n",
    "            if Done == 1:\n",
    "                count.append(i)\n",
    "                rewards.append(total_reward)\n",
    "                break\n",
    "                \n",
    "            allowed_a = np.copy(allowed_a_next)\n",
    "            X = np.copy(X_next)\n",
    "\n",
    "            i += 1  # UPDATE COUNTER FOR NUMBER OF ACTIONS\n",
    "    return count, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "network_replay = Network(hidden_layer=256, input_dim=regular_input_size, output_dim=regular_n_possible_actions, eta=0.02)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f171a02",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "replay_count, replay_rewards = replay(environment=regular_chess_environment, network=network_replay, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.4, B=0.0001, y=0.7)\n",
    "f'Experience Replay Agent training, Average reward:{np.mean(replay_rewards)}. Number of steps: {np.mean(replay_count)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nr_moves_er_test, rewards_er_test, stucks_er_test = test(network=network_replay, environment=testing_chess_environment, number_of_episodes=NUMBER_OF_TEST_EPISODES)\n",
    "f'Experience Replay Agent testing, Average reward: {np.mean(rewards_er_test)}. Number of steps: {np.mean(nr_moves_er_test)}. Nr of stucks: {stucks_er_test}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "replay_ema_reward = pd.DataFrame(replay_rewards).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Experience Replay Reward / Game\")\n",
    "plt.plot(replay_ema_reward, label=\"Experience Replay\", color=REPLAY_COLOR)\n",
    "plt.plot(sarsa_ema_reward, label=\"Sarsa\", color=SARSA_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de0c0a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "replay_ema_moves = pd.DataFrame(replay_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Experience Replay Moves / Game\")\n",
    "plt.plot(replay_ema_moves, label=\"Experience Replay\", color=REPLAY_COLOR)\n",
    "plt.plot(sarsa_ema_moves, label=\"Sarsa\", color=SARSA_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e7525e",
   "metadata": {},
   "source": [
    "## Task 6a\n",
    "We changed the administration of rewards as follows: Every regular action / move gets a slight punishment of -0.01, while a checkmate results in a reward of 1.0 and stale in a punishment of -1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "improved_network = Network(hidden_layer=256, input_dim=improved_input_size, output_dim=improved_n_possible_actions, eta=0.02)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "improved_count, improved_rewards = replay(environment=improved_chess_environment, network=improved_network, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.4, B=0.0001, y=0.7)\n",
    "f'Improved Replay Agent training, Average reward: {np.mean(improved_rewards)}. Number of steps: {np.mean(improved_count)}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nr_moves_improved, reward_improved, stuck_improved = test(network=improved_network, environment=testing_chess_environment, number_of_episodes=NUMBER_OF_TEST_EPISODES)\n",
    "f'Improved Replay Agent testing, Average reward: {np.mean(reward_improved)}. Number of steps: {np.mean(nr_moves_improved)}. Nr of stucks: {stuck_improved}',"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "improved_ema_reward = pd.DataFrame(improved_rewards).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Improved Replay Reward / Game\")\n",
    "plt.plot(improved_ema_reward, label=\"Improved Replay\", color=IMPROVED_REPLAY_COLOR)\n",
    "plt.plot(replay_ema_reward, label=\"Replay\", color=REPLAY_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "improved_ema_moves = pd.DataFrame(improved_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Improved Replay Moves / Game\")\n",
    "plt.plot(improved_ema_moves, label=\"Improved Replay\", color=IMPROVED_REPLAY_COLOR)\n",
    "plt.plot(replay_ema_moves, label=\"Replay\", color=REPLAY_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 6b\n",
    "We removed the additional input in the state representation (number of possible moves for opponent king, information about checked-state)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec5e5e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "minimal_network = Network(hidden_layer=256, input_dim=minimal_input_size, output_dim=minimal_n_possible_actions, eta=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be856a06",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "minimal_count, minimal_rewards = qlearn(environment=minimal_chess_environment, network=minimal_network, number_of_episodes=NUMBER_OF_EPISODES, epsilon=0.4, B=0.0001, y=0.7)\n",
    "f'Less-Information Q-Learning Agent training, Average reward: {np.mean(minimal_rewards)}. Number of steps: {np.mean(minimal_count)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nr_moves_minimal, reward_minimal, stuck_minimal = test(network=minimal_network, environment=minimal_chess_environment, number_of_episodes=NUMBER_OF_TEST_EPISODES)\n",
    "f'Less-Information Q-Learning Agent testing, Average reward: {np.mean(reward_minimal)}. Number of steps: {np.mean(nr_moves_minimal)}. Nr of stucks: {stuck_minimal}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "minimal_ema_reward = pd.DataFrame(minimal_rewards).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Less-Information Q-Learning Reward / Game\")\n",
    "plt.plot(minimal_ema_reward, label=\"Reduced Q-Learning\", color=REDUCED_QLEARNING_COLOR)\n",
    "plt.plot(qlearn_ema_reward, label=\"Q-Learning\", color=QLEARNING_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68be98",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "minimal_ema_moves = pd.DataFrame(minimal_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Less-Information Q-Learning Moves / Game\")\n",
    "plt.plot(minimal_ema_moves, label=\"Reduced Q-Learning\", color=REDUCED_QLEARNING_COLOR)\n",
    "plt.plot(qlearn_ema_moves, label=\"Q-Learning\", color=QLEARNING_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea16443f",
   "metadata": {},
   "source": [
    "## Task 7\n",
    "We implemented RMSProp as part of the Network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebce812",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "network_rmsprop = Network(hidden_layer=256, input_dim=regular_input_size, output_dim=regular_n_possible_actions, eta=0.001, rmsprop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8733f6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rmsprop_count, rmsprop_rewards = replay(environment=regular_chess_environment, network=network_rmsprop, epsilon=0.4, number_of_episodes=NUMBER_OF_EPISODES, B=0.0001, y=0.7)\n",
    "f'RMSProp Replay Agent training, Average reward: {np.mean(rmsprop_rewards)}. Number of steps: {np.mean(rmsprop_count)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nr_moves_q3_test, rewards_q3_test, stucks_q3_test = test(network=network_rmsprop, environment=testing_chess_environment, number_of_episodes=NUMBER_OF_TEST_EPISODES)\n",
    "f'RMSProp Replay Agent testing, Average reward: {np.mean(rewards_q3_test)}. Number of steps: {np.mean(nr_moves_q3_test)}. Nr of stucks: {stucks_q3_test}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rmsprop_ema_reward = pd.DataFrame(rmsprop_rewards).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"RMSProp Replay Reward / Game\")\n",
    "plt.plot(rmsprop_ema_reward, label=\"RMSProp Replay\", color=RMSPROP_REPLAY_COLOR)\n",
    "plt.plot(replay_ema_reward, label=\"Replay\", color=REPLAY_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa1204",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rmsprop_ema_moves = pd.DataFrame(rmsprop_count).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"RMSProp Replay Moves / Game\")\n",
    "plt.plot(rmsprop_ema_moves, label=\"RMSProp Replay\", color=RMSPROP_REPLAY_COLOR)\n",
    "plt.plot(replay_ema_moves, label=\"Replay\", color=REPLAY_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overall Comparison\n",
    "\n",
    "## Training: Rewards per Training Episode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rmsprop_ema_reward = pd.DataFrame(rmsprop_rewards).ewm(halflife=1000).mean()\n",
    "plt.figure()\n",
    "plt.title(\"Reward / Game\")\n",
    "plt.plot(sarsa_ema_reward, label=\"Sarsa\", color=SARSA_COLOR)\n",
    "plt.plot(adapted_sarsa_ema_reward, label=\"Adapted Sarsa\", color=ADAPTED_SARSA_COLOR)\n",
    "plt.plot(qlearn_ema_reward, label=\"Q-Learning\", color=QLEARNING_COLOR)\n",
    "plt.plot(minimal_ema_reward, label=\"Reduced Q-Learning\", color=REDUCED_QLEARNING_COLOR)\n",
    "plt.plot(replay_ema_reward, label=\"Replay\", color=REPLAY_COLOR)\n",
    "plt.plot(rmsprop_ema_reward, label=\"RMSProp Replay\", color=RMSPROP_REPLAY_COLOR)\n",
    "plt.plot(improved_ema_reward, label=\"Improved Replay\", color=IMPROVED_REPLAY_COLOR)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training: Number of Moves until Game ends per Training Episode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Moves / Game\")\n",
    "plt.plot(sarsa_ema_moves, label=\"Sarsa\", color=SARSA_COLOR)\n",
    "plt.plot(adapted_sarsa_ema_moves, label=\"Adapted Sarsa\", color=ADAPTED_SARSA_COLOR)\n",
    "plt.plot(qlearn_ema_moves, label=\"Q-Learning\", color=QLEARNING_COLOR)\n",
    "plt.plot(minimal_ema_moves, label=\"Reduced Q-Learning\", color=REDUCED_QLEARNING_COLOR)\n",
    "plt.plot(replay_ema_moves, label=\"Replay\", color=REPLAY_COLOR)\n",
    "plt.plot(rmsprop_ema_moves, label=\"RMSProp Replay\", color=RMSPROP_REPLAY_COLOR)\n",
    "plt.plot(improved_ema_moves, label=\"Improved Replay\", color=IMPROVED_REPLAY_COLOR)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing: Mean Rewards in testing environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agents = (  'Sarsa',        'Adapted Sarsa',     'Q-Learning',      'Reduced Q-Learning',    'Replay',      'RMSProp Replay',       'Improved Replay',      )\n",
    "colors=[    SARSA_COLOR,    ADAPTED_SARSA_COLOR, QLEARNING_COLOR,   REDUCED_QLEARNING_COLOR, REPLAY_COLOR,  RMSPROP_REPLAY_COLOR,   IMPROVED_REPLAY_COLOR   ]\n",
    "y_pos = np.arange(len(agents))\n",
    "performance = np.array([rewards_sarsa_test, rewards_adapted_sarsa_test, rewards_q_test, reward_minimal, rewards_er_test, rewards_q3_test, reward_improved])\n",
    "performance = np.mean(performance, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "hbars = ax.barh(y_pos, performance, align='center', color=colors)\n",
    "ax.set_yticks(y_pos, labels=agents)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Reward')\n",
    "ax.set_title('Reward per Agent (Larger is better)')\n",
    "\n",
    "# Label with specially formatted floats\n",
    "ax.bar_label(hbars, fmt=' %.2f')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing: Mean Number of Moves until Game ends in testing environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "performance = np.array([nr_moves_sarsa_test, nr_moves_adapted_sarsa_test, nr_moves_q_test, nr_moves_minimal, nr_moves_er_test, nr_moves_q3_test, nr_moves_improved])\n",
    "performance = np.mean(performance, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "hbars = ax.barh(y_pos, performance, align='center', color=colors)\n",
    "ax.set_yticks(y_pos, labels=agents)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Steps (Smaller is better)')\n",
    "ax.set_title('Steps per Agent')\n",
    "\n",
    "# Label with specially formatted floats\n",
    "ax.bar_label(hbars, fmt=' %.2f')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-6c1f981b",
   "language": "python",
   "display_name": "PyCharm (ItRL)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}